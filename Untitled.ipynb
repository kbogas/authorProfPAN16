{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from pan import ProfilingDataset\n",
    "from tictacs import from_recipe\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#reload(pan.features)\n",
    "\n",
    "\n",
    "\n",
    "log = []\n",
    "\n",
    "\n",
    "def cross_val(dataset, task, model, num_folds=4):\n",
    "    \"\"\" train and cross validate a model\n",
    "\n",
    "    :lang: the language\n",
    "    :task: the task we want to classify for , ex: age\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #X, y = dataset.get_data(task)\n",
    "    docs = createDocProfiles(dataset)\n",
    "    X, y = create_target_prof_trainset(docs, task)\n",
    "    del docs\n",
    "    #return X\n",
    "    # get parameters for grid search if it exists - else pass empty dict\n",
    "    params = model.grid_params if hasattr(model, 'grid_params') else dict()\n",
    "    print '\\nCreating model for %s - %s' % (dataset.lang, task)\n",
    "    print 'Using %s fold validation' % (num_folds)\n",
    "    # get data\n",
    "    #log.append('\\nResults for %s - %s with classifier %s' %\n",
    "    #           (dataset.lang, task, model.__class__.__name__))\n",
    "    if task in dataset.config.classifier_list:\n",
    "        grid_cv = GridSearchCV(model, params, cv=num_folds, verbose=1,\n",
    "                               n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        accuracy = grid_cv.best_score_\n",
    "        log.append('best params: %s' % grid_cv.best_params_)\n",
    "        log.append('Accuracy mean : %s' % accuracy)\n",
    "    else:\n",
    "        # if it's not, we measure mean square root error (regression)\n",
    "        grid_cv = GridSearchCV(model, params, scoring='mean_squared_error',\n",
    "                               cv=num_folds, verbose=1, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        accuracy = grid_cv.best_score_\n",
    "        log.append('root mean squared error : %s' % accuracy)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description='Train a model with crossvalidation'\n",
    "                            ' on pan dataset - used for testing purposes ')\n",
    "    parser.add_argument('-i', '--input', type=str,\n",
    "                        required=True, dest='infolder',\n",
    "                        help='path to folder with pan dataset for a language')\n",
    "    parser.add_argument('-n', '--numfolds', type=int,\n",
    "                        dest='num_folds', default=4,\n",
    "                        help='Number of folds to use in cross validation')\n",
    "\n",
    "num_folds = 2\n",
    "infolder = \"./pan15-author-profiling-training-dataset-2015-04-23/pan15-author-profiling-training-dataset-english-2015-04-23/\"\n",
    "\n",
    "print('Loading dataset...')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded %s users...\\n' % len(dataset.entries))\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "for task in tasks:\n",
    "    if task == \"age\":\n",
    "        tictac = from_recipe(config.recipes[task])\n",
    "        z = cross_val(dataset, task, tictac, num_folds)\n",
    "        # print results at end\n",
    "        print('\\n--------------- Thy time of Judgement ---------------')\n",
    "    for message in log:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pan\n",
    "reload(pan.preprocess)\n",
    "dataset = ProfilingDataset(infolder)\n",
    "X, y = dataset.get_data(task)\n",
    "b = [X[0][0:100]]\n",
    "b.append(X[1][0:100])\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tictac.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DocProfile(object):\n",
    "    \n",
    "    \"\"\" Per Document Representation. Returns an instance of a document profile.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, entry, prof_id, doc_id):\n",
    "        \"\"\" Initialization.\n",
    "            -entry : contains most information. Comes from ProfilingDataset Class.\n",
    "            -prof_id: index for intra-profile document position\n",
    "            -doc_id: index for global documend indexing\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.userid = entry.userid\n",
    "        self.lang = entry.lang\n",
    "        self.media = entry.media\n",
    "        self.gender = entry.gender\n",
    "        self.age = entry.age\n",
    "        self.prof_id = prof_id\n",
    "        self.doc_id = doc_id\n",
    "        self.text = entry.texts[prof_id]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\" IPython friendly output\n",
    "        :returns: str\n",
    "\n",
    "        \"\"\"\n",
    "        # automatically capture all non iterables\n",
    "        # (we want custom formatting for text list)\n",
    "        attr_string = '\\n'.join(['%s : %s' % (key, value)\n",
    "                                 for key, value in self.__dict__.items()\n",
    "                                 if not hasattr(value, '__iter__')])\n",
    "        # print a snippet\n",
    "        return attr_string\n",
    "    \n",
    "    def datafy(self, feature='none'):\n",
    "        \"\"\"Return a tuple of data - training and label if feature is not none\n",
    "\n",
    "        :feature: the feature we want the label for\n",
    "        :returns: tuple of data, label\n",
    "\n",
    "        \"\"\"\n",
    "        if feature == 'none':\n",
    "            return self.text\n",
    "        else:\n",
    "            return [self.text, self.__dict__[feature]]\n",
    "\n",
    "def createDocProfiles(dataset):\n",
    "    \"\"\" Create a list of the DocProfiles classes.\n",
    "        -dataset: ProfilingDataset Object\n",
    "        \n",
    "        returns:\n",
    "        -a list of DocProfile Objects\n",
    "    \"\"\"\n",
    "    docs = []       \n",
    "    doc_id = 0\n",
    "    for entry in dataset.entries:\n",
    "        for prof_id in range(0, len(entry.texts)):\n",
    "            docs.append(DocProfile(entry, prof_id, doc_id))\n",
    "            doc_id += 1\n",
    "    return docs\n",
    "    \n",
    "def create_target_prof_trainset(docs, target_feature):\n",
    "    \"\"\" Create a dataset according to train a specifici model regardin a certain feature.\n",
    "        Like get_data() method from ProfilingDataset class.\n",
    "        -docs: list of documents. Expects instances of class DocProfile. \n",
    "        -target_feature: filter feature\n",
    "        \n",
    "        returns:\n",
    "        (X,y) : returns tuple - list of texts, list of labels \n",
    "        \n",
    "    \"\"\"\n",
    "    wanted = []\n",
    "    for doc in docs:\n",
    "        if target_feature in doc.__dict__:\n",
    "            wanted.append(doc.datafy(feature=target_feature))\n",
    "        else:\n",
    "            raise KeyError(\"task doesn't exist in DocProfile dic()\")\n",
    "    # zip produces tuples, we want to be able to modify\n",
    "    # the contents in preprocessing in place\n",
    "    # therefore we create we replace tuples with lists using map\n",
    "    # returns tuple - list of texts, list of labels\n",
    "    return map(list, zip(*wanted))\n",
    "\n",
    "        \n",
    "docs = createDocProfiles(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = [[0.25,0.25,0.25,0.25], [0.5,0,0.2,0.25], [0.2,0.3,0,0.5]]\n",
    "b = [[1,0], [0,1],[1,0]]\n",
    "numpy.dot(numpy.array(a).T,numpy.array(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = 'age'\n",
    "docs = createDocProfiles(dataset)\n",
    "X, y = create_target_prof_trainset(docs, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pan.misc import *\n",
    "class SOA_Model2(object):\n",
    "\n",
    "\n",
    "    \"\"\" Models that extracts Second Order Attributes (SOA) base on PAN 2013-2015 Winners\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "        \n",
    "        #stop_list = []\n",
    "        #with open(stopwords_path, 'r') as stop_inp:\n",
    "       # for w in stop_inp:\n",
    "       # stop_list.append(w.replace(\"\\n\", \"\"))\n",
    "        self.term_table = None\n",
    "        self.labels = None\n",
    "        #self.counter = CountVectorizer()\n",
    "        self.counter = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        import numpy\n",
    "        from math import log\n",
    "        \n",
    "        if y:\n",
    "            #tokens = [_twokenize.tokenizeRawTweetText(text) for text in X]\n",
    "            #voc = set()\n",
    "            #for token in tokens:\n",
    "            #    voc = voc.union(token)\n",
    "            #print len(voc)\n",
    "            #print list(voc)[:100]\n",
    "            parameters = {\n",
    "                'input':'content', \n",
    "                'encoding':'utf-8', \n",
    "                'decode_error':'ignore', \n",
    "                #'vocabulary':list(voc),\n",
    "                'tokenizer':lambda text:_twokenize.tokenizeRawTweetText(text)\n",
    "                #'max_df':0.9,\n",
    "                #'min_df':5\n",
    "                #'max_features':20000\n",
    "               }\n",
    "            self.counter.set_params(**parameters) \n",
    "            #print \"Oleeeeeeeeeeeeeeeeeeeeeeeeee\"\n",
    "            #print texts\n",
    "            #print tokens\n",
    "            #print list(voc)\n",
    "            target_profiles = sorted(list(set(y)))\n",
    "            print len(target_profiles)\n",
    "            #return\n",
    "            doc_term = self.counter.fit_transform(X)\n",
    "            print \"Doc_Terms\"\n",
    "            print doc_term.shape\n",
    "            #return \n",
    "            #X1 = X.toarray()\n",
    "            #X1 = X1.astype('float', casting='unsafe')\n",
    "            target_profiles = sorted(list(set(y)))\n",
    "            self.labels = target_profiles\n",
    "            doc_prof = numpy.zeros([doc_term.shape[0], len(target_profiles)])\n",
    "            for i in range(0, doc_term.shape[0]):\n",
    "                tmp = numpy.zeros([1,len(target_profiles)])\n",
    "                tmp[0, target_profiles.index(y[i])] = 1\n",
    "                doc_prof[i,:] = tmp\n",
    "            print \"Doc_Prof\"\n",
    "            print doc_prof.shape\n",
    "            term_prof = numpy.zeros([doc_term.shape[1], len(target_profiles)])\n",
    "            term_prof = numpy.dot(numpy.log2(doc_term.toarray().astype('float', casting='unsafe').T + 1), doc_prof)\n",
    "            print \"Term_Prof\"\n",
    "            print term_prof.shape\n",
    "            term_prof = term_prof / numpy.reshape(term_prof.sum(axis=1), (term_prof.sum(axis=1).shape[0], 1))\n",
    "            #term_prof = term_prof / term_prof.sum(axis=0)\n",
    "            self.term_table = term_prof\n",
    "            print \"GG\"\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        import numpy\n",
    "        \n",
    "        if self.labels==None:\n",
    "            raise AttributeError('term_table was no found! Probably model was not fitted first. Run model.fit(X,y)!')\n",
    "        else:\n",
    "            doc_term = self.counter.transform(X)\n",
    "            doc_prof = numpy.zeros([doc_term.shape[0], self.term_table.shape[1]])\n",
    "            doc_prof = numpy.dot(doc_term.toarray().astype('float', casting='unsafe'), self.term_table)\n",
    "            return doc_prof\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        import numpy\n",
    "        \n",
    "        doc_prof = self.transform(X)\n",
    "        y_pred = []\n",
    "        for i in range(0, doc_prof.shape[0]):\n",
    "            y_pred.append(self.labels[numpy.argmax(doc_prof[i])])\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pan.misc import _twokenize\n",
    "import pan\n",
    "reload(pan)\n",
    "c = SOA_Model2()\n",
    "c.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = c.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in c.counter.vocabulary_.iteritems():\n",
    "    if v>8000 and v< 9000:\n",
    "        pass\n",
    "        #print k, v\n",
    "top_words = [[] for i in range(0, c.term_table.shape[1])]\n",
    "cc= 0\n",
    "for i in range(0, c.term_table.shape[0]):\n",
    "    if max(c.term_table[i]) > 0.7:\n",
    "        #print c.term_table[i], c.counter.vocabulary_.keys()[c.counter.vocabulary_.values().index(i)]\n",
    "        top_words[list(c.term_table[i]).index(max(c.term_table[i]))].append(c.counter.vocabulary_.keys()[c.counter.vocabulary_.values().index(i)])\n",
    "        cc += 1\n",
    "top_words\n",
    "        #c.term_table /= c.term_table[8411].sum(axis= 0)\n",
    "#c.term_table[8411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "tmp = numpy.zeros([2,4])\n",
    "tmp[0,1]=1\n",
    "tmp[0,3]= 1\n",
    "tmp[1,2] = 1\n",
    "tmp / numpy.reshape(tmp.sum(axis=1), (tmp.sum(axis=1).shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pan\n",
    "reload(pan.features)\n",
    "c = pan.features.SOA_Model2()\n",
    "c.fit_transform(X, y)\n",
    "a = [\"I am very good!\"]\n",
    "#c.transform(X, y)\n",
    "#from pprint import pprint\n",
    "#pprint(dataset.get_data()[0])\n",
    "#pprint(dataset.entries[0].texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "aaa = numpy.asarray([[1, 2], [3, 4]], dtype=float)\n",
    "bb = aaa.sum(axis=1)\n",
    "print numpy.reshape(bb, (1,2))\n",
    "print aaa/numpy.reshape(bb, (bb.shape[0],1))\n",
    "print aaa/aaa.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.preprocessing import normalize\n",
    "aaa = numpy.asarray([[1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=float)\n",
    "print \"sci-kit\"\n",
    "cc = normalize(aaa, axis=1, norm='l1')\n",
    "print normalize(aaa, axis=1, norm='l1')\n",
    "print normalize(cc, axis=0, norm='l1')\n",
    "print numpy.sum(aaa,axis=1, keepdims=True)\n",
    "print numpy.linalg.norm(aaa, axis=1)\n",
    "aaa = numpy.true_divide(aaa, numpy.sum(aaa,axis=1, keepdims=True), dtype=float)\n",
    "print numpy.sum(aaa,axis=0, keepdims=True)\n",
    "aaa = numpy.true_divide(aaa, numpy.sum(aaa,axis=0, keepdims=True), dtype=float)\n",
    "print aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pan\n",
    "reload(pan.features)\n",
    "log = []\n",
    "#import logging\n",
    "#log = logging.getLogger()\n",
    "#log.setLevel(logging.INFO)\n",
    "#log.addHandler(logging.StreamHandler())\n",
    "infolder = \"./pan15-author-profiling-training-dataset-2015-04-23/pan15-author-profiling-training-dataset-english-2015-04-23/\"\n",
    "modelfile\n",
    "print('Loading dataset...')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded %s users...\\n' % len(dataset.entries))\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "all_models = {}\n",
    "for task in tasks:\n",
    "    print('Learning to judge %s..' % task)\n",
    "    # load data\n",
    "    if task == \"age\":\n",
    "        X, y = dataset.get_data(task)\n",
    "        tictac = from_recipe(config.recipes[task])\n",
    "        all_models[task] = tictac.fit(X, y)\n",
    "print('Writing model to {}'.format(modelfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tictacs\n",
    "tictacs.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = set()\n",
    "a.add(1)\n",
    "a.add(2)\n",
    "a.add(3)\n",
    "print(a[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "a = CountVectorizer(input=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(pan.features)\n",
    "c = pan.features.SOA_Model()\n",
    "a = [\"I am very good!\"]\n",
    "#aa = c.fit_transform(a)\n",
    "print b\n",
    "c.fit([b])\n",
    "print aa\n",
    "print c.counter.vocabulary_\n",
    "kk = c.transform([b])\n",
    "print kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "            \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    " # remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "          for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "from pprint import pprint   # pretty-printer\n",
    "pprint(texts)\n",
    "dictionary = gensim.corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "model = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=5)\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pow(2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from pan import ProfilingDataset\n",
    "from tictacs import from_recipe\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#reload(pan.features)\n",
    "\n",
    "\n",
    "\n",
    "log = []\n",
    "\n",
    "\n",
    "def cross_val(dataset, task, model, num_folds=4):\n",
    "    \"\"\" train and cross validate a model\n",
    "\n",
    "    :lang: the language\n",
    "    :task: the task we want to classify for , ex: age\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #X, y = dataset.get_data(task)\n",
    "    docs = createDocProfiles(dataset)\n",
    "    X, y = create_target_prof_trainset(docs, task)\n",
    "    del docs\n",
    "    #return X\n",
    "    # get parameters for grid search if it exists - else pass empty dict\n",
    "    params = model.grid_params if hasattr(model, 'grid_params') else dict()\n",
    "    print '\\nCreating model for %s - %s' % (dataset.lang, task)\n",
    "    print 'Using %s fold validation' % (num_folds)\n",
    "    # get data\n",
    "    #log.append('\\nResults for %s - %s with classifier %s' %\n",
    "    #           (dataset.lang, task, model.__class__.__name__))\n",
    "    if task in dataset.config.classifier_list:\n",
    "        grid_cv = GridSearchCV(model, params, cv=num_folds, verbose=1,\n",
    "                               n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        accuracy = grid_cv.best_score_\n",
    "        log.append('best params: %s' % grid_cv.best_params_)\n",
    "        log.append('Accuracy mean : %s' % accuracy)\n",
    "    else:\n",
    "        # if it's not, we measure mean square root error (regression)\n",
    "        grid_cv = GridSearchCV(model, params, scoring='mean_squared_error',\n",
    "                               cv=num_folds, verbose=1, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        accuracy = grid_cv.best_score_\n",
    "        log.append('root mean squared error : %s' % accuracy)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description='Train a model with crossvalidation'\n",
    "                            ' on pan dataset - used for testing purposes ')\n",
    "    parser.add_argument('-i', '--input', type=str,\n",
    "                        required=True, dest='infolder',\n",
    "                        help='path to folder with pan dataset for a language')\n",
    "    parser.add_argument('-n', '--numfolds', type=int,\n",
    "                        dest='num_folds', default=4,\n",
    "                        help='Number of folds to use in cross validation')\n",
    "\n",
    "num_folds = 2\n",
    "infolder = \"./pan15-author-profiling-training-dataset-2015-04-23/pan15-author-profiling-training-dataset-english-2015-04-23/\"\n",
    "\n",
    "print('Loading dataset...')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded %s users...\\n' % len(dataset.entries))\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "for task in tasks:\n",
    "    if task == \"age\":\n",
    "        tictac = from_recipe(config.recipes[task])\n",
    "        z = cross_val(dataset, task, tictac, num_folds)\n",
    "        # print results at end\n",
    "        print('\\n--------------- Thy time of Judgement ---------------')\n",
    "    for message in log:\n",
    "        print(message)\n",
    "\n",
    "\n",
    "all_models = {}\n",
    "docs = createDocProfiles(dataset)\n",
    "for task in tasks:\n",
    "    if task =='age':\n",
    "        print('Learning to judge %s..' % task)\n",
    "        # load data\n",
    "        X, y = create_target_prof_trainset(docs, task)\n",
    "        #X, y = dataset.get_data(task)\n",
    "        tictac = from_recipe(config.recipes[task])\n",
    "        all_models[task] = tictac.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "          for document in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model =gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=100, minimum_probability=0)\n",
    "a = model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "lsi[corpus[2]]\n",
    "import numpy\n",
    "#len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = lsi[corpus]\n",
    "l = [list(zip(*cc)[1]) for cc in c]\n",
    "#l = []\n",
    "#for cc in c:\n",
    "    #print cc\n",
    "#    l.append(list(zip(*cc)[1]))\n",
    "print numpy.array(l)\n",
    "    #print list(zip(*cc)[1])\n",
    "    #for k in cc:\n",
    "    #    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pan.features\n",
    "reload(pan.features)\n",
    "pan.features.TWCNB.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from argparse import ArgumentParser\n",
    "from pan import ProfilingDataset, createDocProfiles, create_target_prof_trainset\n",
    "from tictacs import from_recipe\n",
    "from json import dumps\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "log = []\n",
    "\n",
    "\n",
    "def cross_val(dataset, task, model, num_folds=4):\n",
    "    \"\"\" train and cross validate a model\n",
    "\n",
    "    :lang: the language\n",
    "    :task: the task we want to classify for , ex: age\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # if (task != \"age\") and (task !=\"gender\"):\n",
    "    #    X, y = dataset.get_data(task)\n",
    "    # else:\n",
    "    #    docs = createDocProfiles(dataset)\n",
    "    #    X, y = create_target_prof_trainset(docs, task)\n",
    "    X, y = dataset.get_data(task)\n",
    "    # y = [yy.lower() for yy in y]\n",
    "    # get parameters for grid search if it exists - else pass empty dict\n",
    "    params = model.grid_params if hasattr(model, 'grid_params') else dict()\n",
    "    # from collections import Counter\n",
    "    # import pprint\n",
    "    # pprint.pprint(Counter(y))\n",
    "    print '\\nCreating model for %s - %s' % (dataset.lang, task)\n",
    "    print 'Trainining instances: %s\\n' % (len(X))\n",
    "    print 'Using %s fold validation' % (num_folds)\n",
    "    # get data\n",
    "    log.append('\\nResults for %s - %s with classifier %s' %\n",
    "               (dataset.lang, task, model.__class__.__name__))\n",
    "    if task in dataset.config.classifier_list:\n",
    "        grid_cv = GridSearchCV(model, params, cv=num_folds, verbose=1,\n",
    "                               n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        # y_pred = grid_cv.best_estimator_.predict(X)\n",
    "        # pprint.pprint(y_pred)\n",
    "        # pprint.pprint(y)\n",
    "        # conf = confusion_matrix(y, y_pred, labels=list(set(y)))\n",
    "        accuracy = grid_cv.best_score_\n",
    "        # accuracy2 = accuracy_score(y, y_pred)\n",
    "        log.append('best params: %s' % grid_cv.best_params_)\n",
    "        log.append('Accuracy mean : %s' % accuracy)\n",
    "        import pprint\n",
    "        pprint.pprint(grid_cv.grid_scores_)\n",
    "        with open('./comb_res/res.txt', 'a') as out:\n",
    "            out.write(' Results: %s - %s, params: %s ,Accuracy_Mean: %s\\n' %\n",
    "                      (dataset.lang, task,\n",
    "                       dumps(grid_cv.best_params_), grid_cv.best_score_))\n",
    "        # log.append('Best accuracy: {} '.format(accuracy2))\n",
    "        # log.append('Best Confusion matrix :\\n {}'.format(conf))\n",
    "    else:\n",
    "        # if it's not, we measure mean square root error (regression)\n",
    "        raise KeyError('task %s was not found in task list!' % task)\n",
    "\n",
    "\n",
    "\n",
    "infolder = '../DATA/pan16-author-profiling-training-dataset-2016-02-29/pan16-author-profiling-training-dataset-english-2016-02-29/'\n",
    "num_folds = 3\n",
    "time_start = time.time()\n",
    "print('Loading dataset...')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded %s users...\\n' % len(dataset.entries))\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "for task in tasks:\n",
    "    tictac = from_recipe(config.recipes[task])\n",
    "    import pprint\n",
    "    #pprint.pprint(tictac.__dict__)\n",
    "    #exit(1)\n",
    "    steps = tictac.steps\n",
    "    #print type(steps)\n",
    "    outline = \"\"\n",
    "    for step in steps:\n",
    "        if step[0]==\"features\":\n",
    "            # print type(step[1])\n",
    "            for tf in step[1].transformer_list:\n",
    "                #print type(tf[1])\n",
    "                #print type(tf[1].get_params())\n",
    "                outline += tf[0] + \" with Params:[\" + str(tf[1].get_params()) + \"]+\"\n",
    "        else:\n",
    "#            if hasattr(step[1], 'get_params'):\n",
    "#                outline += step[0] + \" with Params:[\" + str(step[1].get_params()) + \"]+\"\n",
    "#            else:\n",
    "#                outline += step[0]+ \"+\"\n",
    "            outline += step[0]+ \"+\"\n",
    "    outline = outline[:-1] + \"\\n\"\n",
    "    print('Task:{}, Pipeline:{}'.format(task, outline))\n",
    "    with open('./comb_res/res.txt', 'a') as out:\n",
    "        out.write('Task:{}, Pipeline:{}'.format(task, outline))\n",
    "    cross_val(dataset, task, tictac, num_folds)\n",
    "# print results at end\n",
    "print('\\n--------------- Thy time of Judgement ---------------')\n",
    "print ('Time: {} seconds.\\n'.format(str(time.time()-time_start)))\n",
    "with open('./comb_res/res.txt', 'a') as out:\n",
    "    out.write('Time: {} seconds.\\n'.format(str(time.time()-time_start)))\n",
    "for message in log:\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.pickles(tictac)\n",
    "dill.detect.badtypes(tictac).__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### TRAIN ############\n",
    "\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.externals import joblib\n",
    "from tictacs import from_recipe\n",
    "from pan import ProfilingDataset\n",
    "import dill\n",
    "import cPickle as pickle\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "infolder = \"../pan16-author-profiling-training-dataset-2016-02-29/pan16-author-profiling-training-dataset-english-2016-02-29/\"\n",
    "outfolder = \"models/\"\n",
    "print('Loading dataset->Grouping User texts.\\n')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded {} users...\\n'.format(len(dataset.entries)))\n",
    "# get config\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "all_models = {}\n",
    "for task in tasks:\n",
    "    print('Learning to judge %s..' % task)\n",
    "    # load data\n",
    "    X, y = dataset.get_data(task)\n",
    "    tictac = from_recipe(config.recipes[task])\n",
    "    all_models[task] = tictac.fit(X, y)\n",
    "modelfile = os.path.join(outfolder, '%s2.bin' % dataset.lang)\n",
    "print('Writing model to {}'.format(modelfile))\n",
    "#fo = open(modelfile,  'wb')\n",
    "#import pprint\n",
    "#print type(all_models)\n",
    "#print modelfile\n",
    "#dill.dump(all_models, fo, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#fo.close()\n",
    "# pickle.dump(all_models, modelfile)\n",
    "# dill.dump(all_models, modelfile)\n",
    "joblib.dump(all_models, modelfile, compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "a = numpy.array([[1,2],[3,4]], dtype=float)\n",
    "b = numpy.array([[0.1,0.2],[0.3,0.4]], dtype=float)\n",
    "type(a[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = numpy.array([1,2,3,4])\n",
    "print a.shape\n",
    "b = numpy.tile(a, (5, 1))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = b.sum(axis=1)\n",
    "print c.shape, type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import pprint\n",
    "pprint.pprint(a)\n",
    "normalize(a, norm='l1', axis=1, copy=False)\n",
    "pprint.pprint(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset->Grouping User texts.\n",
      "\n",
      "Loaded 436 users...\n",
      "\n",
      "\n",
      "--------------- Thy time of Running ---------------\n",
      "Learning to judge age..\n",
      "Learning to judge gender..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.externals import joblib\n",
    "from tictacs import from_recipe\n",
    "from pan import ProfilingDataset\n",
    "import dill\n",
    "import cPickle as pickle\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "infolder = \"../DATA/pan16-author-profiling-training-dataset-2016-04-25/pan16-author-profiling-training-dataset-english-2016-02-29/\"\n",
    "outfolder = \"models/\"\n",
    "print('Loading dataset->Grouping User texts.\\n')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded {} users...\\n'.format(len(dataset.entries)))\n",
    "# get config\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "all_models = {}\n",
    "for task in tasks:\n",
    "    print('Learning to judge %s..' % task)\n",
    "    # load data\n",
    "    X, y = dataset.get_data(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples: 436\n",
      "Counter({'35-49': 182, '25-34': 140, '50-64': 80, '18-24': 28, '65-xx': 6})\n",
      "436\n",
      "87 87 262 436\n",
      "Counter({'35-49': 182, '25-34': 140, '50-64': 80, '18-24': 28, '65-xx': 6})\n",
      "Counter({'35-49': 36, '25-34': 28, '50-64': 16, '18-24': 6, '65-xx': 1})\n",
      "Counter({'35-49': 37, '25-34': 28, '50-64': 16, '18-24': 5, '65-xx': 1})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "import pprint\n",
    "print \"Num of samples: \" + str(len(y))\n",
    "pprint.pprint(Counter(y))\n",
    "X, y = dataset.get_data('age')\n",
    "print len(X)\n",
    "\n",
    "X, X_cv, X, y_cv = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_cv, y_cv, test_size=0.5, random_state=42, stratify=y_cv)\n",
    "\n",
    "print len(X_cv), len(X_test), len(X) , len(X)+ len(X_cv) + len(X_test)\n",
    "pprint.pprint(Counter(y))\n",
    "pprint.pprint(Counter(y_cv))\n",
    "pprint.pprint(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "    -Cleaning html\n",
      "    -Detwittifying\n",
      "    -Removing Numbers\n",
      "    -Removing Punctuation\n",
      "    -Removing Links\n"
     ]
    }
   ],
   "source": [
    "#reload(preprocess)\n",
    "#reload(features)\n",
    "from pan import features\n",
    "from pan import preprocess\n",
    "X, y = dataset.get_data('age')\n",
    "#X, y = dataset.get_data('gender')\n",
    "print len(X)\n",
    "#print X[0]\n",
    "X = preprocess.preprocess(X)\n",
    "#print \"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\"\n",
    "#print X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3grams+soa+soac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('combined', FeatureUnion(n_jobs=1,\n",
       "         transformer_list=[('3grams', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "          dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "          lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "          ngram_range=[3, 3], norm=u'l2', preprocessor=None, smo...ulary=None)), ('soa', SOA_Model2(max_df=1.0, max_features=None, min_df=5, tokenizer_var='sklearn'))],\n",
       "         transformer_weights=None)),\n",
       " ('svm', SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "grams3 = TfidfVectorizer(analyzer='word', ngram_range=[3,3], max_features=5000, stop_words='english')\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=None)\n",
    "soac = features.SOAC_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=5000)\n",
    "countTokens = features.CountTokens()\n",
    "countHash = features.CountHash()\n",
    "countUrls = features.CountURLs()\n",
    "countReplies = features.CountReplies()\n",
    "svm = SVC(kernel='rbf', C=1, gamma=1, class_weight='balanced', probability=True)\n",
    "#svm = DecisionTreeClassifier()\n",
    "combined = FeatureUnion([('3grams', grams3), ('soa', soa)])\n",
    "pipe = Pipeline([('combined',combined), ('svm', svm)])\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts + SOA+SOAC. Ommit preprocess!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Complementary of SOA model 22'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(features)\n",
    "features.SOAC_Model2.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3grams',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "          dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "          lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "          ngram_range=[3, 3], norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "grams3 = TfidfVectorizer(analyzer='word', ngram_range=[3,3], max_features=5000, stop_words='english')\n",
    "countTokens = features.CountTokens()\n",
    "countHash = features.CountHash()\n",
    "countUrls = features.CountURLs()\n",
    "countReplies = features.CountReplies()\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=None)\n",
    "soac = features.SOAC_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=5000)\n",
    "scaler = StandardScaler()#MinMaxScaler()#StandardScaler()\n",
    "#svm = DecisionTreeClassifier()\n",
    "svm = SVC(kernel='rbf', C=0.1, gamma=1, class_weight='balanced', probability=True)\n",
    "#combined = FeatureUnion([('soa', soa), ('soac', soac)])\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies), \n",
    "#                          ('soa', soa), ('soac', soac)])\n",
    "combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "pipe = Pipeline([('3grams', grams3), ('svm', svm)])\n",
    "#pipe = Pipeline([('soac',soac), ('svm', svm)])\n",
    "#pipe = Pipeline([('combined',combined), ('svm', svm)])\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pan.features import LDA\n",
    "\n",
    "LDAmodel = LDA(num_topics=30, lib='sklearn')\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=None)\n",
    "soac = features.SOAC_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=5000)\n",
    "countTokens = features.CountTokens()\n",
    "countHash = features.CountHash()\n",
    "countUrls = features.CountURLs()\n",
    "countReplies = features.CountReplies()\n",
    "#svm = SVC(kernel='rbf', C=1, gamma=1, class_weight='balanced')\n",
    "svm = DecisionTreeClassifier()\n",
    "combined = FeatureUnion([('LDA', LDAmodel)])#, ('soa', soa), ('soac', soac)])\n",
    "pipe = Pipeline([('combined',combined), ('svm', svm)])\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3grams+soa', FeatureUnion(n_jobs=1,\n",
       "         transformer_list=[('3grams', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "          dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "          lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "          ngram_range=[3, 3], norm=u'l2', preprocessor=None, smo...ulary=None)), ('soa', SOA_Model2(max_df=1.0, max_features=None, min_df=1, tokenizer_var='sklearn'))],\n",
       "         transformer_weights=None)),\n",
       " ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from pan.features import SOA_Model2\n",
    "\n",
    "grams3 = TfidfVectorizer(analyzer='word', ngram_range=[3,3], max_features=5000, stop_words='english')\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=1, tokenizer_var='sklearn', max_features=None)\n",
    "combined = FeatureUnion([('3grams', grams3), ('soa', soa)])\n",
    "svm = SVC(kernel='rbf', C=0.1, gamma=1, class_weight='balanced', probability=True)\n",
    "pipe = Pipeline([('3grams+soa',combined), ('svm', svm)])\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soac', SOAC_Model2(max_df=1.0, max_features=None, min_df=1, thres=0.1,\n",
       "        tokenizer_var='sklearn')),\n",
       " ('svm', SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soac = features.SOAC_Model2(max_df=1.0, min_df=1, tokenizer_var='sklearn', max_features=None)\n",
    "svm = SVC(kernel='rbf', C=10, gamma=1, class_weight='balanced', probability=True)\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies), \n",
    "#                          ('soa', soa), ('soac', soac)])\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "pipe1 = Pipeline([('soac',soac), ('svm', svm)])\n",
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('LDAmodel', LDA(lib='sklearn', num_topics=30)),\n",
       " ('svm', SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pan.features import LDA\n",
    "LDAmodel = LDA(num_topics=30, lib='sklearn')\n",
    "countTokens = features.CountTokens()\n",
    "countHash = features.CountHash()\n",
    "countUrls = features.CountURLs()\n",
    "countReplies = features.CountReplies()\n",
    "combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "svm = SVC(kernel='rbf', C=10, gamma=1, class_weight='balanced', probability=True)\n",
    "#pipe2 = Pipeline([('counts',combined), ('svm', svm)])\n",
    "pipe2 = Pipeline([('LDAmodel',LDAmodel), ('svm', svm)])\n",
    "pipe2.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3grams',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "          dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "          lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "          ngram_range=[3, 3], norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('svm', SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from pan.features import SOA_Model2\n",
    "\n",
    "grams3 = TfidfVectorizer(analyzer='word', ngram_range=[3,3], max_features=5000, stop_words='english')\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=1, tokenizer_var='sklearn', max_features=None)\n",
    "#combined = FeatureUnion([('3grams', grams3), ('soa', soa)])\n",
    "svm = SVC(kernel='rbf', C=10, gamma=1, class_weight='balanced', probability=True)\n",
    "pipe = Pipeline([('3grams',grams3), ('svm', svm)])\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soac', SOAC_Model2(max_df=1.0, max_features=None, min_df=1, thres=0.1,\n",
       "        tokenizer_var='sklearn')),\n",
       " ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pan.features import SOAC_Model2\n",
    "soac = features.SOAC_Model2(max_df=1.0, min_df=1, tokenizer_var='sklearn', max_features=None)\n",
    "svm = SVC(kernel='rbf', C=0.1, gamma=1, class_weight='balanced', probability=True)\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies), \n",
    "#                          ('soa', soa), ('soac', soac)])\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "pipe1 = Pipeline([('soac',soac), ('svm', svm)])\n",
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('soa',\n",
       "  SOA_Model2(max_df=1.0, max_features=None, min_df=1, tokenizer_var='sklearn')),\n",
       " ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pan.features import LDA\n",
    "LDAmodel = LDA(num_topics=120, lib='sklearn')\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=1, tokenizer_var='sklearn', max_features=None)\n",
    "countTokens = features.CountTokens()\n",
    "countHash = features.CountHash()\n",
    "countUrls = features.CountURLs()\n",
    "countReplies = features.CountReplies()\n",
    "combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "svm = SVC(kernel='rbf', C=0.1, gamma=1, class_weight='balanced', probability=True)\n",
    "#pipe2 = Pipeline([('counts',combined), ('svm', svm)])\n",
    "#pipe2 = Pipeline([('LDAmodel',LDAmodel), ('svm', svm)])\n",
    "pipe2 = Pipeline([('soa',soa), ('svm', svm)])\n",
    "pipe2.steps                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, copy\n",
    "\n",
    "def print_overlaps(predictions, names, verbose=True):\n",
    "    N = len(names)\n",
    "    res = numpy.zeros([N,N])\n",
    "    for i in range(0, N):\n",
    "        for j in range(i+1, N):\n",
    "            #print i,j\n",
    "            #predictions[i]\n",
    "            #predictions[j]\n",
    "            res[i,j] = len([(k,v) for k,v in zip(predictions[i], predictions[j]) if k==v])/float(len(predictions[0]))\n",
    "            #print res[i,j]\n",
    "            if verbose:\n",
    "                print \"%s - %s : %0.3f  overlap\" % (names[i],  names[j], 100*res[i,j])\n",
    "    return  res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 262\n",
      "259 259\n",
      "87 87\n",
      "86 86\n",
      "87 87\n",
      "87 87\n",
      "259 87 86 432 436\n",
      "Split: 0.3\n",
      "301 66 367 436\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearnsklearn\n",
      "sklearn\n",
      "sklearn\n",
      "\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearnsklearn\n",
      "sklearn\n",
      "sklearn\n",
      "\n",
      "sklearn\n",
      "Fit took: 20.095 seconds\n",
      "Predict took: 99.007 seconds\n",
      "['18-24', '35-49', '50-64', '25-34', '65-xx']\n",
      "['3grams+soa', 'lda', 'soac']\n",
      "3grams+soa\n",
      "lda\n",
      "soac\n",
      "(65, 15) (65,)\n",
      "fit true\n",
      "3grams+soa\n",
      "lda\n",
      "soac\n",
      "Predict\n",
      "array([1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1,\n",
      "       1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1,\n",
      "       1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1])\n",
      "array(['25-34', '25-34', '25-34', '25-34', '25-34', '25-34', '35-49',\n",
      "       '35-49', '25-34', '25-34', '25-34', '35-49', '25-34', '25-34',\n",
      "       '25-34', '25-34', '35-49', '35-49', '25-34', '35-49', '25-34',\n",
      "       '25-34', '25-34', '25-34', '35-49', '25-34', '25-34', '35-49',\n",
      "       '25-34', '25-34', '35-49', '35-49', '25-34', '25-34', '25-34',\n",
      "       '25-34', '25-34', '25-34', '25-34', '25-34', '25-34', '35-49',\n",
      "       '25-34', '35-49', '25-34', '25-34', '25-34', '35-49', '25-34',\n",
      "       '25-34', '35-49', '35-49', '25-34', '25-34', '25-34', '25-34',\n",
      "       '25-34', '25-34', '25-34', '25-34', '35-49', '25-34', '25-34',\n",
      "       '25-34', '35-49', '25-34'], \n",
      "      dtype='|S5')\n",
      "302 65 367 436\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearnsklearn\n",
      "sklearn\n",
      "sklearn\n",
      "\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearn\n",
      "sklearnsklearn\n",
      "sklearn\n",
      "sklearn\n",
      "\n",
      "sklearn\n",
      "Fit took: 18.390 seconds\n",
      "Predict took: 95.106 seconds\n",
      "['18-24', '35-49', '50-64', '25-34', '65-xx']\n",
      "['3grams+soa', 'lda', 'soac']\n",
      "3grams+soa\n",
      "lda\n",
      "soac\n",
      "(65, 15) (65,)\n",
      "fit true\n",
      "3grams+soa\n",
      "lda\n",
      "soac\n",
      "Predict\n",
      "array([1, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1,\n",
      "       4, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 4, 1, 1, 4, 2, 1, 1,\n",
      "       1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 4, 1, 1, 1, 1])\n",
      "array(['25-34', '65-xx', '65-xx', '65-xx', '25-34', '25-34', '25-34',\n",
      "       '25-34', '25-34', '25-34', '25-34', '25-34', '25-34', '35-49',\n",
      "       '25-34', '25-34', '25-34', '25-34', '25-34', '65-xx', '35-49',\n",
      "       '25-34', '25-34', '65-xx', '25-34', '25-34', '65-xx', '25-34',\n",
      "       '25-34', '25-34', '25-34', '25-34', '35-49', '25-34', '25-34',\n",
      "       '25-34', '25-34', '65-xx', '25-34', '65-xx', '25-34', '25-34',\n",
      "       '65-xx', '35-49', '25-34', '25-34', '25-34', '65-xx', '25-34',\n",
      "       '25-34', '25-34', '25-34', '25-34', '25-34', '25-34', '25-34',\n",
      "       '25-34', '25-34', '65-xx', '25-34', '65-xx', '25-34', '25-34',\n",
      "       '25-34', '25-34'], \n",
      "      dtype='|S5')\n",
      "302 65 367 436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from pan.features import Metaclassifier\n",
    "import time\n",
    "\n",
    "#pipe = Pipeline([('3grams',grams3), ('svm', svm)])\n",
    "#pipe1 = Pipeline([('soac',soac), ('svm', svm)])\n",
    "#pipe2 = Pipeline([('soa',soa), ('svm', svm)])\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, random_state=42, stratify=y)\n",
    "X_meta, X_cv, y_meta, y_cv = train_test_split(X_cv, y_cv, test_size=0.5, stratify=y_cv)\n",
    "\n",
    "print len(y_train), len(X_train)\n",
    "for i, x in enumerate(X_train):\n",
    "    if len(x)==0:\n",
    "        X_train.remove(x)\n",
    "        y_train.remove(y_train[i])\n",
    "print len(y_train), len(X_train)\n",
    "\n",
    "\n",
    "print len(y_cv), len(X_cv)\n",
    "for i, x in enumerate(X_cv):\n",
    "    if len(x)==0:\n",
    "        X_cv.remove(x)\n",
    "        y_cv.remove(y_cv[i])\n",
    "print len(y_cv), len(X_cv)\n",
    "\n",
    "print len(y_meta), len(X_meta)\n",
    "for i, x in enumerate(X_meta):\n",
    "    if len(x)==0:\n",
    "        X_meta.remove(x)\n",
    "        y_meta.remove(y_meta[i])\n",
    "print len(y_meta), len(X_meta)    \n",
    "\n",
    "print len(X_train),len(X_meta),len(X_cv),len(X_train)+len(X_cv)+len(X_meta), len(X)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[(\"0\", pipe), ('1', pipe1), (\"2\", pipe2)], voting='soft')\n",
    "eclfh = VotingClassifier(estimators=[(\"0\", pipe), ('1', pipe1), (\"2\", pipe2)], voting='hard')\n",
    "models = [pipe,pipe1,pipe2,eclf, eclfh]\n",
    "model_names = ['3grams+soa', 'soac', 'lda', 'voting', 'votingh']\n",
    "results = {}\n",
    "for name in model_names:\n",
    "    results[name] = {'pred': [], 'acc': [], 'conf': [], 'over': []}\n",
    "results['space'] = {'pred': [], 'acc': [], 'conf': [], 'over':[]}\n",
    "results['meta'] = {'pred': [], 'acc': [], 'conf': [], 'over':[]}\n",
    "params = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "params = {}\n",
    "num_folds = 3\n",
    "splits = [0.3, 0.4]\n",
    "N = 4\n",
    "t0 = time.time()\n",
    "for split in splits:\n",
    "    print \"Split: \" + str(split)  \n",
    "    for i in xrange(N):\n",
    "        #X, y = dataset.get_data('age')\n",
    "        #X, y = dataset.get_data('gender')\n",
    "        X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, stratify=y)\n",
    "        for i, x in enumerate(X_train):\n",
    "            if len(x)==0:\n",
    "                X_train.remove(x)\n",
    "                y_train.remove(y_train[i])\n",
    "        for i, x in enumerate(X_cv):\n",
    "            if len(x)==0:\n",
    "                X_cv.remove(x)\n",
    "                y_cv.remove(y_cv[i])\n",
    "        if 'space' or 'meta' in results.keys():\n",
    "            X_meta, X_cv, y_meta, y_cv = train_test_split(X_cv, y_cv, test_size=0.5, stratify=y_cv)\n",
    "        print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "        trained_models = []\n",
    "        for i, model in enumerate(models):\n",
    "            if model_names[i] == 'voting' or model_names[i] == 'votingh':\n",
    "                params = {}\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=params, verbose=0, n_jobs=-1, cv=num_folds, refit=True)\n",
    "            grid_search.fit(X_train,y_train)\n",
    "            trained_models.append(grid_search.best_estimator_)\n",
    "        predictions = []\n",
    "        for i, model in enumerate(trained_models):\n",
    "            predict = model.predict(X_cv)\n",
    "            predictions.append(predict)\n",
    "            results[model_names[i]]['pred'].append(predict)\n",
    "            results[model_names[i]]['acc'].append(accuracy_score(y_cv, predict))\n",
    "            results[model_names[i]]['conf'].append(confusion_matrix(y_cv, predict, labels=list(set(y))))\n",
    "        # Space model ###\n",
    "        models_for_space = {}\n",
    "        cv_scores = []\n",
    "        for name, model in zip(model_names, trained_models):\n",
    "            if name!='voting' and name!='votingh':\n",
    "                models_for_space[name] = model\n",
    "                cv_scores.append(model.score(X_meta, y_meta))\n",
    "        space = SubSpaceEnsemble3(models_for_space, cv_scores)\n",
    "        space.fit(X_train + X_meta, y_train+y_meta)\n",
    "        predict = space.predict(X_cv)\n",
    "        #grid_search = GridSearchCV(space, param_grid={}, verbose=0, n_jobs=-1, cv=num_folds, refit=True)\n",
    "        #grid_search.fit(X_meta+X_train, y_meta+y_train)\n",
    "        #predict = grid_search.best_estimator_.predict(X_cv)\n",
    "        results['space']['pred'].append(predict)\n",
    "        results['space']['acc'].append(accuracy_score(y_cv, predict))\n",
    "        results['space']['conf'].append(confusion_matrix(y_cv, predict, labels=list(set(y))))\n",
    "        predictions.append(predict)\n",
    "        # Space model end ###\n",
    "        # Meta ###\n",
    "        model_dic = {}\n",
    "        for i, model in enumerate(trained_models):\n",
    "            if model_names[i] != 'voting' and model_names[i] !='votingh': \n",
    "                model_dic[model_names[i]] = model\n",
    "        Meta = Metaclassifier(models=model_dic, C=1.0, weights='balanced')\n",
    "        Meta.fit(X_meta, y_meta)\n",
    "        predict = Meta.predict(X_cv)\n",
    "        results['meta']['pred'].append(predict)\n",
    "        results['meta']['acc'].append(accuracy_score(y_cv, predict))\n",
    "        results['meta']['conf'].append(confusion_matrix(y_cv, predict, labels=list(set(y))))\n",
    "        predictions.append(predict)\n",
    "        # Meta model END ###\n",
    "        predictions.append(y_cv)\n",
    "        results['3grams+soa']['over'].append(print_overlaps(predictions, model_names+['space', 'meta', 'true'], False))\n",
    "    print('Split %0.1f.: %0.3f seconds') % (split, time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37878787878787878, 0.34482758620689657]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['3grams']['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 0.3\n",
      "----------- Scores-----------\n",
      "\n",
      "Model: 3grams+soa Accuracy: 0.269 Std: 0.207\n",
      "\n",
      "Model: soac Accuracy: 0.331 Std: 0.054\n",
      "\n",
      "Model: lda Accuracy: 0.415 Std: 0.000\n",
      "\n",
      "Model: voting Accuracy: 0.369 Std: 0.000\n",
      "\n",
      "Model: votingh Accuracy: 0.338 Std: 0.109\n",
      "\n",
      "Model: space Accuracy: 0.369 Std: 0.044\n",
      "\n",
      "Model: meta Accuracy: 0.315 Std: 0.120\n",
      "----------- Overlaps-----------\n",
      "3grams+soa - soac : 24.615  overlap\n",
      "3grams+soa - lda : 50.000  overlap\n",
      "3grams+soa - voting : 36.154  overlap\n",
      "3grams+soa - votingh : 79.231  overlap\n",
      "3grams+soa - space : 33.846  overlap\n",
      "3grams+soa - meta : 16.154  overlap\n",
      "3grams+soa - true : 26.923  overlap\n",
      "soac - lda : 40.000  overlap\n",
      "soac - voting : 80.769  overlap\n",
      "soac - votingh : 45.385  overlap\n",
      "soac - space : 74.615  overlap\n",
      "soac - meta : 68.462  overlap\n",
      "soac - true : 33.077  overlap\n",
      "lda - voting : 59.231  overlap\n",
      "lda - votingh : 70.769  overlap\n",
      "lda - space : 61.538  overlap\n",
      "lda - meta : 30.769  overlap\n",
      "lda - true : 41.538  overlap\n",
      "voting - votingh : 56.923  overlap\n",
      "voting - space : 83.846  overlap\n",
      "voting - meta : 66.923  overlap\n",
      "voting - true : 36.923  overlap\n",
      "votingh - space : 53.846  overlap\n",
      "votingh - meta : 35.385  overlap\n",
      "votingh - true : 33.846  overlap\n",
      "space - meta : 58.462  overlap\n",
      "space - true : 36.923  overlap\n",
      "meta - true : 31.538  overlap\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "Split: 0.4\n",
      "----------- Scores-----------\n",
      "\n",
      "Model: 3grams+soa Accuracy: 0.236 Std: 0.251\n",
      "\n",
      "Model: soac Accuracy: 0.251 Std: 0.095\n",
      "\n",
      "Model: lda Accuracy: 0.419 Std: 0.007\n",
      "\n",
      "Model: voting Accuracy: 0.394 Std: 0.109\n",
      "\n",
      "Model: votingh Accuracy: 0.272 Std: 0.201\n",
      "\n",
      "Model: space Accuracy: 0.355 Std: 0.047\n",
      "\n",
      "Model: meta Accuracy: 0.274 Std: 0.111\n",
      "----------- Overlaps-----------\n",
      "3grams+soa - soac : 11.521  overlap\n",
      "3grams+soa - lda : 50.000  overlap\n",
      "3grams+soa - voting : 41.379  overlap\n",
      "3grams+soa - votingh : 88.235  overlap\n",
      "3grams+soa - space : 35.057  overlap\n",
      "3grams+soa - meta : 22.110  overlap\n",
      "3grams+soa - true : 23.631  overlap\n",
      "soac - lda : 22.110  overlap\n",
      "soac - voting : 64.861  overlap\n",
      "soac - votingh : 23.286  overlap\n",
      "soac - space : 53.016  overlap\n",
      "soac - meta : 23.604  overlap\n",
      "soac - true : 25.078  overlap\n",
      "lda - voting : 54.909  overlap\n",
      "lda - votingh : 61.765  overlap\n",
      "lda - space : 59.763  overlap\n",
      "lda - meta : 45.639  overlap\n",
      "lda - true : 41.866  overlap\n",
      "voting - votingh : 53.144  overlap\n",
      "voting - space : 67.424  overlap\n",
      "voting - meta : 21.332  overlap\n",
      "voting - true : 39.446  overlap\n",
      "votingh - space : 44.469  overlap\n",
      "votingh - meta : 10.345  overlap\n",
      "votingh - true : 27.160  overlap\n",
      "space - meta : 29.101  overlap\n",
      "space - true : 35.504  overlap\n",
      "meta - true : 27.417  overlap\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "for i, split in enumerate(splits):\n",
    "    print 'Split: %0.1f' % split\n",
    "    print '----------- Scores-----------'\n",
    "    for name in model_names + ['space'] + ['meta']:\n",
    "        tmp = results[name]['acc'][N*i:(N*i+N)]\n",
    "        print \n",
    "        print 'Model: %s Accuracy: %0.3f Std: %0.3f' % (name, statistics.mean(tmp), \n",
    "                                                          statistics.stdev(tmp))\n",
    "        #tmp_conf = copy.deepcopy(results[name]['conf'][N*i])\n",
    "        #for j in xrange(N*i+1, N*i+N):\n",
    "        #    tmp_conf += results[name]['conf'][j]\n",
    "        #tmp_conf /= N\n",
    "        #print('Confusion matrix :\\n {}'.format(tmp_conf))\n",
    "    print '----------- Overlaps-----------'\n",
    "    tmp_overlaps = copy.deepcopy(results['3grams+soa']['over'][N*i])\n",
    "    for j in xrange(N*i+1, N*i+N):\n",
    "            tmp_overlaps += results['3grams+soa']['over'][j]\n",
    "    tmp_overlaps /= N\n",
    "    print_names = model_names+['space', 'meta','true']\n",
    "    for k in xrange(tmp_overlaps.shape[0]):\n",
    "        for v in xrange(k+1, tmp_overlaps.shape[0]):\n",
    "            print \"%s - %s : %0.3f  overlap\" % (print_names[k],  print_names[v], 100*tmp_overlaps[k, v])\n",
    "    print '%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "num_folds = 4\n",
    "split = 0.2\n",
    "#X, y = dataset.get_data('age')\n",
    "#X, y = dataset.get_data('gender')\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, random_state=42, stratify=y)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "eclf = VotingClassifier(estimators=[(\"0\", pipe), ('1', pipe1), (\"2\", pipe2)], voting='soft')\n",
    "models = [pipe,pipe1,pipe2,eclf]\n",
    "model_names = ['3grams', 'soac', 'lda', 'voting']\n",
    "trained_models = []\n",
    "for i, model in enumerate(models):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_estimator_) \n",
    "    trained_models.append(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SubSpaceEnsemble(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\" A Linear Weights Metaclassifier \"\"\"\n",
    "\n",
    "    def __init__(self, models, cv_scores):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        if (not models) or (not cv_scores):\n",
    "            raise AttributeError('Models expexts a dictonary of models \\\n",
    "              containg the predictions of y_true for each classifier.\\\n",
    "              cv_score expects a list len(models.keys()) with the\\\n",
    "              cross validation scores of each model')\n",
    "        else:\n",
    "            self.models = models\n",
    "            self.cv_scores = cv_scores\n",
    "            self.ind2names = {}\n",
    "            for i, name in enumerate(models.keys()):\n",
    "                self.ind2names[i] = name\n",
    "            self.counter = CountVectorizer()\n",
    "            self.doc_terms = None\n",
    "            self.experts = []\n",
    "        \n",
    "\n",
    "    def fit(self, X_cv, y_true=None, weights=None):\n",
    "        \n",
    "        import random\n",
    "\n",
    "        if y_true is None:\n",
    "            raise ValueError('we need y labels to supervise-fit!')\n",
    "        else:\n",
    "            parameters = {\n",
    "                    'input': 'content',\n",
    "                    'encoding': 'utf-8',\n",
    "                    'decode_error': 'ignore',\n",
    "                    'analyzer': 'word',\n",
    "                    'stop_words': 'english',\n",
    "                    # 'vocabulary':list(voc),\n",
    "                    #'tokenizer': tokenization,\n",
    "                    #'tokenizer': _twokenize.tokenizeRawTweetText,  # self.tokenization,\n",
    "                    #'tokenizer': lambda text: _twokenize.tokenizeRawTweetText(nonan.sub(po_re.sub('', text))),\n",
    "                    'max_df': 1.0,\n",
    "                    'min_df': 1,\n",
    "                    'max_features':None\n",
    "                }\n",
    "            self.counter.set_params(**parameters)\n",
    "            self.doc_terms = self.counter.fit_transform(X_cv).toarray()\n",
    "            predictions = []\n",
    "            for name, model in self.models.iteritems():\n",
    "                predictions.append(model.predict(X_cv))\n",
    "            count = 0\n",
    "            for i, y in enumerate(y_true):\n",
    "                possible_experts = []\n",
    "                for j, pred in enumerate(predictions):\n",
    "                    if pred[i] == y:\n",
    "                        possible_experts.append(j)\n",
    "                if possible_experts:\n",
    "                    possible_scores = [self.cv_scores[poss] for poss in possible_experts]\n",
    "                    self.experts.append(possible_experts[possible_scores.index(max(possible_scores))])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    self.experts.append(self.cv_scores.index(max(self.cv_scores)))\n",
    "            print \"Chosen through expert: %0.2f\" % (100*count/float(len(y_true))) \n",
    "            #print self.expert_scores\n",
    "            #print self.experts\n",
    "            return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # print \"PRedict\"\n",
    "        # print X.shape\n",
    "        X_transformed = self.counter.transform(X).toarray()\n",
    "        #print type((X_transformed)[0])\n",
    "        #print X_transformed.shape\n",
    "        #return 0\n",
    "        y_pred = []\n",
    "        for i in range(0, X_transformed.shape[0]):\n",
    "            #print X_transformed[i,:].shape\n",
    "            best_model_ind = self.find_sim_projection(X_transformed[i,:])\n",
    "            #print best_model_ind\n",
    "            #print self.models[self.ind2names[best_model_ind]].predict([X[i]])[0]\n",
    "            y_pred.append(self.models[self.ind2names[best_model_ind]].predict([X[i]])[0])\n",
    "        #print y_pred\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X), normalize=True)\n",
    "        #return self.svc.score(self.transform_to_y(X), y, sample_weight)\n",
    "\n",
    "\n",
    "    def find_sim_projection(self, x_sample):\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        cos = []\n",
    "        j = None\n",
    "        min_s = -10000\n",
    "        for i in range(0, self.doc_terms.shape[0]):\n",
    "            #print x_sample.reshape(1,-1).shape\n",
    "            #print self.doc_terms[i,:].reshape(1,-1).shape\n",
    "            temp = cosine_similarity(x_sample.reshape(1,-1), self.doc_terms[i,:].reshape(1,-1))[0][0]\n",
    "            if min_s < 0 or  temp > min_s:\n",
    "                min_s = temp\n",
    "                j = i\n",
    "        return self.experts[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SubSpaceEnsemble2(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\" A Linear Weights Metaclassifier \"\"\"\n",
    "\n",
    "    def __init__(self, models, cv_scores, k=10):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        if (not models) or (not cv_scores):\n",
    "            raise AttributeError('Models expexts a dictonary of models \\\n",
    "              containg the predictions of y_true for each classifier.\\\n",
    "              cv_score expects a list len(models.keys()) with the\\\n",
    "              cross validation scores of each model')\n",
    "        else:\n",
    "            self.models = models\n",
    "            self.cv_scores = cv_scores\n",
    "            self.k = k\n",
    "            self.ind2names = {}\n",
    "            for i, name in enumerate(models.keys()):\n",
    "                self.ind2names[i] = name\n",
    "            self.counter = CountVectorizer()\n",
    "            self.predictions = []\n",
    "            self.true = []\n",
    "            self.doc_terms = None\n",
    "            self.tree = None\n",
    "            self.experts = []\n",
    "        \n",
    "\n",
    "    def fit(self, X_cv, y_true=None, weights=None):\n",
    "        \n",
    "        from sklearn.neighbors import BallTree\n",
    "        import random\n",
    "\n",
    "        if y_true is None:\n",
    "            raise ValueError('we need y labels to supervise-fit!')\n",
    "        else:\n",
    "            parameters = {\n",
    "                    'input': 'content',\n",
    "                    'encoding': 'utf-8',\n",
    "                    'decode_error': 'ignore',\n",
    "                    'analyzer': 'word',\n",
    "                    'stop_words': 'english',\n",
    "                    # 'vocabulary':list(voc),\n",
    "                    #'tokenizer': tokenization,\n",
    "                    #'tokenizer': _twokenize.tokenizeRawTweetText,  # self.tokenization,\n",
    "                    #'tokenizer': lambda text: _twokenize.tokenizeRawTweetText(nonan.sub(po_re.sub('', text))),\n",
    "                    'max_df': 1.0,\n",
    "                    'min_df': 1,\n",
    "                    'max_features':None\n",
    "                }\n",
    "            self.counter.set_params(**parameters)\n",
    "            self.doc_terms = self.counter.fit_transform(X_cv).toarray()\n",
    "            self.tree = BallTree(self.doc_terms, leaf_size=20)\n",
    "            predictions = []\n",
    "            for name, model in self.models.iteritems():\n",
    "                predictions.append(model.predict(X_cv))\n",
    "            self.predictions = predictions\n",
    "            self.true = y_true\n",
    "            count = 0\n",
    "            #print self.expert_scores\n",
    "            #print self.experts\n",
    "            return self\n",
    "\n",
    "    def predict(self, X, y_real):\n",
    "        \n",
    "\n",
    "        # print \"PRedict\"\n",
    "        # print X.shape\n",
    "        X_transformed = self.counter.transform(X).toarray()\n",
    "        #print type((X_transformed)[0])\n",
    "        #print X_transformed.shape\n",
    "        #return 0\n",
    "        y_pred = []\n",
    "        for i in range(0, X_transformed.shape[0]):\n",
    "            #print X_transformed[i,:].shape\n",
    "            dist, neigbors_indexes = self.tree.query(X_transformed[i,:].reshape(1,-1), self.k)  \n",
    "            print 'Sample ' + y_real[i]\n",
    "            #print neigbors_indexes[0]\n",
    "            #print dist\n",
    "            #best_model_ind = self.expert_decision(neigbors_indexes[0])\n",
    "            y_pred.append(self.expert_decision(neigbors_indexes[0],  X[i]))\n",
    "            \n",
    "            #y_pred.append(self.models[self.ind2names[best_model_ind]].predict([X[i]])[0])\n",
    "        #print y_pred\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X), normalize=True)\n",
    "        #return self.svc.score(self.transform_to_y(X), y, sample_weight)\n",
    "\n",
    "\n",
    "    def expert_decision(self, neigbors_indexes, x_sample):\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from collections import Counter\n",
    "        \n",
    "        models_pred = []\n",
    "        acc = []\n",
    "        neigbors_true = [self.true[n_i] for n_i in neigbors_indexes]\n",
    "        print 'True'\n",
    "        print neigbors_true\n",
    "        sample_predictions = []\n",
    "        for model_i in xrange(len(self.models.values())):\n",
    "            model_pred = []\n",
    "            for n_i in neigbors_indexes:\n",
    "                model_pred.append(self.predictions[model_i][n_i])\n",
    "            models_pred.append(model_pred)\n",
    "            acc.append(accuracy_score(neigbors_true, model_pred, normalize=True))\n",
    "            sample_predictions.append(self.models[self.ind2names[model_i]].predict(x_sample)[0])\n",
    "            print 'Model: ' + self.ind2names[model_i] + ' Accuracy: ' + str(accuracy_score(neigbors_true, model_pred, normalize=True))\n",
    "            print 'Predictions'\n",
    "            print model_pred\n",
    "            print 'Sample prediction: ' + str(sample_predictions[model_i])\n",
    "        total_pred = []\n",
    "        weights = {}\n",
    "        weights['true'] = 3\n",
    "        weights['models_n'] = [int(2/float((1-acc_m)+0.01)) for acc_m in acc]\n",
    "        weights['models'] = [int(6/float((1-acc_m)+0.01)) for acc_m in acc]\n",
    "        for i, model in enumerate(models_pred):\n",
    "            if acc[i]>0.35:\n",
    "                for k in model:\n",
    "                    #print weights['models_n'][i]\n",
    "                    total_pred.extend([k for j in xrange(weights['models_n'][i])])\n",
    "                total_pred.extend([sample_predictions[model_i] for j in xrange(weights['models'][i])])\n",
    "        for n in neigbors_true:\n",
    "            total_pred.extend([n for j in xrange(weights['true'])])                     \n",
    "        data = Counter(total_pred)\n",
    "        #data = Counter([k for pred in models_pred for k in pred])\n",
    "        print data\n",
    "        best_model_ind = acc.index(max(acc))\n",
    "        print 'Total pred: ' + str(data.most_common(1)[0][0])\n",
    "        print len(total_pred)\n",
    "        #return best_model_ind\n",
    "        return data.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-dbfd8c31226b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers, not list"
     ]
    }
   ],
   "source": [
    "a = [0,1,2,3]\n",
    "b= [3,1]\n",
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import time\n",
    "\n",
    "class SubSpaceEnsemble3(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\" A Linear Weights Metaclassifier \"\"\"\n",
    "\n",
    "    def __init__(self, models, cv_scores, k=3, weights= [6,3,2,0.7]):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        if (not models) or (not cv_scores):\n",
    "            raise AttributeError('Models expexts a dictonary of models \\\n",
    "              containg the predictions of y_true for each classifier.\\\n",
    "              cv_score expects a list len(models.keys()) with the\\\n",
    "              cross validation scores of each model')\n",
    "        else:\n",
    "            self.models = models\n",
    "            self.cv_scores = cv_scores\n",
    "            self.k = k\n",
    "            self.weights = weights\n",
    "            self.ind2names = {}\n",
    "            for i, name in enumerate(models.keys()):\n",
    "                self.ind2names[i] = name\n",
    "            self.counter = CountVectorizer()\n",
    "            self.representations = []\n",
    "            self.meta = None\n",
    "            self.predictions = []\n",
    "            self.true = []\n",
    "            self.doc_terms = None\n",
    "            self.tree = None\n",
    "            self.experts = []\n",
    "        \n",
    "\n",
    "    def fit(self, X_cv, y_true=None, weights=None):\n",
    "        \n",
    "        from sklearn.neighbors import BallTree\n",
    "        import random\n",
    "\n",
    "        if y_true is None:\n",
    "            raise ValueError('we need y labels to supervise-fit!')\n",
    "        else:\n",
    "            parameters = {\n",
    "                    'input': 'content',\n",
    "                    'encoding': 'utf-8',\n",
    "                    'decode_error': 'ignore',\n",
    "                    'analyzer': 'word',\n",
    "                    'stop_words': 'english',\n",
    "                    # 'vocabulary':list(voc),\n",
    "                    #'tokenizer': tokenization,\n",
    "                    #'tokenizer': _twokenize.tokenizeRawTweetText,  # self.tokenization,\n",
    "                    #'tokenizer': lambda text: _twokenize.tokenizeRawTweetText(nonan.sub(po_re.sub('', text))),\n",
    "                    'max_df': 1.0,\n",
    "                    'min_df': 1,\n",
    "                    'max_features':None\n",
    "                }\n",
    "            t0 = time.time()\n",
    "            self.counter.set_params(**parameters)\n",
    "            self.doc_terms = self.counter.fit_transform(X_cv).toarray()\n",
    "            self.tree = BallTree(self.doc_terms, leaf_size=20)\n",
    "            predictions = []\n",
    "            for name, model in self.models.iteritems():\n",
    "                predictions.append(model.predict(X_cv))\n",
    "                #print len(predictions[-1])\n",
    "                transf = model.steps[0][1].transform(X_cv)\n",
    "                if hasattr(transf, \"toarray\"):\n",
    "                    #print 'Exei'\n",
    "                    self.representations.append(transf.toarray())\n",
    "                else:\n",
    "                    self.representations.append(transf)\n",
    "            self.predictions = predictions\n",
    "            self.true = y_true\n",
    "            count = 0\n",
    "            #print self.expert_scores\n",
    "            #print self.experts\n",
    "            print('Fit took: %0.3f seconds') % (time.time()-t0)\n",
    "            return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "\n",
    "        # print \"PRedict\"\n",
    "        # print X.shape\n",
    "        X_transformed = self.counter.transform(X).toarray()\n",
    "        #print type((X_transformed)[0])\n",
    "        #print X_transformed.shape\n",
    "        #return 0\n",
    "        y_pred = []\n",
    "        t0 = time.time()\n",
    "        for i in range(0, X_transformed.shape[0]):\n",
    "            #print X_transformed[i,:].shape\n",
    "            dist, neigbors_indexes = self.tree.query(X_transformed[i,:].reshape(1,-1), self.k)  \n",
    "            #print 'Sample ' + y_real[i]\n",
    "            #print neigbors_indexes[0]\n",
    "            #print dist\n",
    "            #best_model_ind = self.expert_decision(neigbors_indexes[0])\n",
    "            y_pred.append(self.expert_decision(neigbors_indexes[0],  X[i]))\n",
    "            \n",
    "            #y_pred.append(self.models[self.ind2names[best_model_ind]].predict([X[i]])[0])\n",
    "        #print y_pred\n",
    "        print('Predict took: %0.3f seconds') % (time.time()-t0)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X), normalize=True)\n",
    "        #return self.svc.score(self.transform_to_y(X), y, sample_weight)\n",
    "\n",
    "\n",
    "    def expert_decision(self, neigbors_indexes, x_sample):\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from collections import Counter\n",
    "        from sklearn.neighbors import BallTree\n",
    "        \n",
    "        models_pred = []\n",
    "        models_neig_pred = []\n",
    "        acc = []\n",
    "        t0 = time.time()\n",
    "        neigbors_true = [self.true[n_i] for n_i in neigbors_indexes]\n",
    "        #print('Neighbors per sample: %0.4f seconds') % (time.time()-t0)\n",
    "        #print 'True'\n",
    "        #print neigbors_true\n",
    "        sample_predictions = []\n",
    "        total_pred = []\n",
    "        weights = {}\n",
    "        weights['true'] = self.weights[1]\n",
    "        weights['models_n'] = []\n",
    "        weights['models'] = []\n",
    "        for model_i in xrange(len(self.models.values())):\n",
    "            ModelTree = BallTree(self.representations[model_i])\n",
    "            temp_trans = self.models[self.ind2names[model_i]].steps[0][1].transform([x_sample])\n",
    "            if hasattr(temp_trans, 'toarray'):\n",
    "                temp_trans = temp_trans.toarray()\n",
    "            _, model_neig = ModelTree.query(temp_trans, self.k)\n",
    "            model_neig_pred = []\n",
    "            for model_n_i in model_neig[0].tolist():\n",
    "                model_neig_pred.append(self.predictions[model_i][model_n_i])\n",
    "            models_neig_pred.append(model_neig_pred)\n",
    "            model_pred = []\n",
    "            for n_i in neigbors_indexes:\n",
    "                model_pred.append(self.predictions[model_i][n_i])\n",
    "            models_pred.append(model_pred)\n",
    "            acc.append(accuracy_score(neigbors_true, model_pred, normalize=True))\n",
    "            if acc[-1] >self.weights[3]:\n",
    "                # Adding neighbors predictions\n",
    "                weights['models_n'].append(int(self.weights[2]/float((1-acc[-1])+0.01)))\n",
    "                total_pred.extend([pred for j in xrange(weights['models_n'][-1]) for pred in model_pred])\n",
    "                #print('Predicting Neighbors per sample: %0.4f seconds') % (time.time()-t0)\n",
    "                # Adding sample prediction\n",
    "                sample_predictions.append(self.models[self.ind2names[model_i]].predict(x_sample)[0])\n",
    "                weights['models'].append(int(self.weights[0]/float((1-acc[-1])+0.01))) \n",
    "                total_pred.extend([sample_predictions[-1] for j in xrange(weights['models'][-1])])\n",
    "                total_pred.extend([pred for j in xrange(weights['models'][-1]) for pred in model_neig_pred])\n",
    "            #print len(x_sample)\n",
    "            #print self.ind2names[model_i]\n",
    "            \n",
    "                #print 'Model: ' + self.ind2names[model_i] + ' Accuracy: ' + str(accuracy_score(neigbors_true, model_pred, normalize=True))\n",
    "                #print 'Predictions'\n",
    "                #print model_pred\n",
    "                #print 'Representations'\n",
    "                #print model_neig_pred\n",
    "                #print 'Sample prediction: ' + str(sample_predictions[-1])\n",
    "        total_pred.extend([n for j in xrange(int(weights['true'])) for n in neigbors_true])\n",
    "        #print('creating votes: %0.4f seconds') % (time.time()-t0)\n",
    "        data = Counter(total_pred)\n",
    "        #data = Counter([k for pred in models_pred for k in pred])\n",
    "        #print data\n",
    "        best_model_ind = acc.index(max(acc))\n",
    "        #print 'Total pred: ' + str(data.most_common(1)[0][0])\n",
    "        #print len(total_pred)\n",
    "        #return best_model_ind\n",
    "        return data.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03333333,  0.03333333,  0.03333333, ...,  0.03333333,\n",
       "         0.03333333,  0.03333333],\n",
       "       [ 0.03333333,  0.03333333,  0.03333333, ...,  0.03333333,\n",
       "         0.03333333,  0.03333333],\n",
       "       [ 0.03333333,  0.03333333,  0.03333333, ...,  0.03333333,\n",
       "         0.03333333,  0.03333333],\n",
       "       ..., \n",
       "       [ 0.03333333,  0.03333333,  0.03333333, ...,  0.03333333,\n",
       "         0.03333333,  0.03333333],\n",
       "       [ 0.03333333,  0.03333333,  0.03333333, ...,  0.03333333,\n",
       "         0.03333333,  0.03333333],\n",
       "       [ 0.03333333,  0.03333333,  0.03333333, ...,  0.03333333,\n",
       "         0.03333333,  0.03333333]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = models['lda']\n",
    "a.steps[0][1].transform(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(soa, 'transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 87\n",
      "87 87\n",
      "86 86\n",
      "86 86\n",
      "262 87 349 436\n",
      "Fit took: 17.170 seconds\n",
      "Predict took: 197.758 seconds\n",
      "Accuracy : 0.51724137931\n",
      "Confusion matrix :\n",
      " [[ 1  2  0  2  0]\n",
      " [ 0 30  0  7  0]\n",
      " [ 0 13  0  3  0]\n",
      " [ 0 14  0 14  0]\n",
      " [ 0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "cv_scores = []\n",
    "print len(y_cv), len(X_cv)\n",
    "for i, x in enumerate(X_cv):\n",
    "    if len(x)==0:\n",
    "        X_cv.remove(x)\n",
    "        y_cv.remove(y_cv[i])\n",
    "print len(y_cv), len(X_cv)\n",
    "\n",
    "print len(y_meta), len(X_meta)\n",
    "for i, x in enumerate(X_meta):\n",
    "    if len(x)==0:\n",
    "        X_meta.remove(x)\n",
    "        y_meta.remove(y_meta[i])\n",
    "print len(y_meta), len(X_meta)        \n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "\n",
    "for name, model in zip(model_names, trained_models):\n",
    "    if name!='voting' and name!='votingh' and name!='space' and name!='meta':\n",
    "        models[name] = model\n",
    "        cv_scores.append(model.score(X_meta, y_meta))\n",
    "        \n",
    "w = [1,1,1,0.35]\n",
    "space = SubSpaceEnsemble3(models,cv_scores,k=10, weights=w)\n",
    "space.fit(X_meta+X_train, y_meta+y_train)\n",
    "predict = space.predict(X_cv)\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=list(set(y_cv)))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = numpy.array([[  0, 207,  65, 161,  11,  61, 152,  37, 302,  25]])\n",
    "a[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 88\n",
      "88 88\n",
      "88 88\n",
      "87 87\n",
      "260 88 348 436\n",
      "['lda', 'soac', '3grams']\n",
      "[0.79545454545454541, 0.88636363636363635, 0.88636363636363635]\n",
      "Weights\n",
      "[ 6.    2.    1.    0.35]\n",
      "Fit took: 7.689 seconds\n",
      "Predict took: 127.131 seconds\n",
      "Score: 0.402298850575\n",
      "Weights\n",
      "[ 6.00000001  2.          1.          0.35      ]\n",
      "Fit took: 7.803 seconds\n",
      "Predict took: 128.827 seconds\n",
      "Score: 0.402298850575\n",
      "Weights\n",
      "[ 6.          2.00000001  1.          0.35      ]\n",
      "Fit took: 7.926 seconds\n",
      "Predict took: 127.187 seconds\n",
      "Score: 0.402298850575\n",
      "Weights\n",
      "[ 6.          2.          1.00000001  0.35      ]\n",
      "Fit took: 7.761 seconds\n",
      "Predict took: 127.905 seconds\n",
      "Score: 0.402298850575\n",
      "Weights\n",
      "[ 6.          2.          1.          0.35000001]\n",
      "Fit took: 8.134 seconds\n",
      "Predict took: 128.011 seconds\n",
      "Score: 0.402298850575\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "global X_train, X_meta, x_cv, y_train, y_meta, y_cv\n",
    "\n",
    "def f(w):\n",
    "    print \"Weights\"\n",
    "    print w\n",
    "    space = SubSpaceEnsemble3(models,cv_scores,k=10, weights=w)\n",
    "    space.fit(X_train + X_cv, y_train + y_cv)\n",
    "    score = 1- accuracy_score(y_meta, space.predict(X_meta))\n",
    "    print 'Score: ' + str(score)\n",
    "    return score\n",
    "    \n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, random_state=42, stratify=y)\n",
    "X_meta, X_cv, y_meta, y_cv = train_test_split(X_cv, y_cv, test_size=0.5, stratify=y_cv)\n",
    "\n",
    "models = {}\n",
    "cv_scores = []\n",
    "print len(y_cv), len(X_cv)\n",
    "for i, x in enumerate(X_cv):\n",
    "    if len(x)==0:\n",
    "        X_cv.remove(x)\n",
    "        y_cv.remove(y_cv[i])\n",
    "print len(y_cv), len(X_cv)\n",
    "\n",
    "print len(y_meta), len(X_meta)\n",
    "for i, x in enumerate(X_meta):\n",
    "    if len(x)==0:\n",
    "        X_meta.remove(x)\n",
    "        y_meta.remove(y_meta[i])\n",
    "print len(y_meta), len(X_meta)        \n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "\n",
    "for name, model in zip(model_names, trained_models):\n",
    "    if name!='voting' and name!='space' and name!='meta':\n",
    "        models[name] = model\n",
    "        cv_scores.append(model.score(X_cv, y_cv))\n",
    "\n",
    "print models.keys()\n",
    "print cv_scores\n",
    "#space = SubSpaceEnsemble(models, cv_scores)\n",
    "w = [6,2,1,0.35]\n",
    "bnds = ((0, None), (0, None), (0, None), (0, 1))\n",
    "a = minimize(f, w, bounds=bnds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "cv_scores = []\n",
    "for name, model in zip(model_names, trained_models):\n",
    "    if name!='voting' and name!='space' and name!='meta':\n",
    "        models[name] = model\n",
    "        cv_scores.append(model.score(X_meta, y_meta))\n",
    "\n",
    "print models.keys()\n",
    "print cv_scores\n",
    "space = SubSpaceEnsemble2(models, cv_scores)\n",
    "space.fit(X_meta+X_train,y_meta+y_train)\n",
    "predict = space.predict(X_cv, y_cv)\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=list(set(y_cv)))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('0', Pipeline(steps=[('3grams', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=[3, 3], norm=u'l2', prepro...  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]))],\n",
       "         voting='soft', weights=None)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['voting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lda', 'soac', '3grams']\n",
      "[0.43678160919540232, 0.47126436781609193, 0.51724137931034486]\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Chosen through expert: 66.67\n",
      "Chosen through expert: 66.67Chosen through expert: 63.79\n",
      "Chosen through expert: 70.69\n",
      "Chosen through expert: 65.52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   26.6s finished\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "cv_scores = []\n",
    "for name, model in zip(model_names, trained_models):\n",
    "    if name!='voting' and name!='space' and name!='meta':\n",
    "        models[name] = model\n",
    "        cv_scores.append(model.score(X_cv, y_cv))\n",
    "\n",
    "print models.keys()\n",
    "print cv_scores\n",
    "space = SubSpaceEnsemble(models, cv_scores)\n",
    "grid_search = GridSearchCV(SubSpaceEnsemble(models, cv_scores), param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "grid_search.fit(X_cv, y_cv)\n",
    "space.fit(X_cv, y_cv)\n",
    "y_space = grid_search.best_estimator_.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_space[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.563218390805\n",
      "Confusion matrix :\n",
      " [[14 14  0  0  0]\n",
      " [ 3 34  0  0  0]\n",
      " [ 1 14  1  0  0]\n",
      " [ 1  4  0  0  0]\n",
      " [ 0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predict = y_space\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=list(set(y)))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86824314212445919"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "a = numpy.array([1,2])\n",
    "b = numpy.array([3,2])\n",
    "cosine_similarity(a,b)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.333333333333\n",
      "Confusion matrix :\n",
      " [[ 0 17  0  8  3]\n",
      " [ 0 29  0  6  2]\n",
      " [ 0 11  0  4  1]\n",
      " [ 0  5  0  0  0]\n",
      " [ 0  1  0  0  0]]\n",
      "Accuracy : 0.448275862069\n",
      "Confusion matrix :\n",
      " [[11 17  0  0  0]\n",
      " [10 27  0  0  0]\n",
      " [ 1 14  1  0  0]\n",
      " [ 1  4  0  0  0]\n",
      " [ 0  1  0  0  0]]\n",
      "Accuracy : 0.448275862069\n",
      "Confusion matrix :\n",
      " [[12 16  0  0  0]\n",
      " [10 26  1  0  0]\n",
      " [ 1 14  1  0  0]\n",
      " [ 1  4  0  0  0]\n",
      " [ 0  1  0  0  0]]\n",
      "Accuracy : 0.494252873563\n",
      "Confusion matrix :\n",
      " [[15 13  0  0  0]\n",
      " [10 27  0  0  0]\n",
      " [ 2 13  1  0  0]\n",
      " [ 2  3  0  0  0]\n",
      " [ 0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i, model in enumerate(trained_models):\n",
    "    predict = model.predict(X_cv)\n",
    "    predictions.append(predict)\n",
    "    acc = accuracy_score(y_cv, predict)\n",
    "    conf = confusion_matrix(y_cv, predict, labels=list(set(y)))\n",
    "    print('Accuracy : {}'.format(acc))\n",
    "    print('Confusion matrix :\\n {}'.format(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy, copy\n",
    "\n",
    "def print_overlaps(predictions, names, verbose=True):\n",
    "    N = len(names)\n",
    "    res = numpy.zeros([N,N])\n",
    "    for i in range(0, N):\n",
    "        for j in range(i+1, N):\n",
    "            #print i,j\n",
    "            #predictions[i]\n",
    "            #predictions[j]\n",
    "            res[i,j] = len([(k,v) for k,v in zip(predictions[i], predictions[j]) if k==v])/float(len(predictions[0]))\n",
    "            #print res[i,j]\n",
    "            if verbose:\n",
    "                print \"%s - %s : %0.3f  overlap\" % (names[i],  names[j], 100*res[i,j])\n",
    "    return  res\n",
    "\n",
    "#pred2 = copy.deepcopy(predictions)\n",
    "#pred2.append(y_space)\n",
    "\n",
    "#pred2.append(y_cv)\n",
    "#model_names = ['3grams', 'soac', 'lda', 'voting']\n",
    "#model_names += ['space']\n",
    "#model_names += ['True']\n",
    "#print len([(i, j) for i,j in zip(predictions[0], predictions[1]) if i==j])/float(len(predictions[0]))\n",
    "#print_overlaps(pred2, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 88 432 436\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(344, 5) <type 'numpy.ndarray'>\n",
      "344 344\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (88,5) (88,4) (88,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-e147eec03593>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \"\"\"\n\u001b[1;32m--> 575\u001b[1;33m         \u001b[0mpredicted_probabilitiy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m         return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n\u001b[0;32m    577\u001b[0m                                   axis=0)\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 self.n_classes_)\n\u001b[1;32m--> 621\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36m_parallel_predict_proba\u001b[1;34m(estimators, estimators_features, X, n_classes)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mproba\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mproba_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (88,5) (88,4) (88,5) "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "svm = SVC(kernel='rbf', C=10, gamma=1, class_weight='balanced', probability=True)\n",
    "\n",
    "#clf = AdaBoostClassifier(base_estimator=svm, n_estimators=100, learning_rate=1.0, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "clf = BaggingClassifier(base_estimator = svm, n_estimators=100, verbose=1, random_state=42)\n",
    "\n",
    "#X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, random_state=42, stratify=y)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "X_train_new = soac.transform(X_train)\n",
    "#for i, x in enumerate(X_train):\n",
    "#    if len(x)<=1 or y_train[i]<=1:\n",
    "#        print 'y'\n",
    "#        X_train.remove(x)\n",
    "#        y_train.remove(y_train[i])\n",
    "print len(X_train), len(y_train)\n",
    "clf.fit(X_train_new,y_train)\n",
    "predict= clf.predict(soac.transform(X_cv))\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=list(set(y)))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3grams</th>\n",
       "      <th>soac</th>\n",
       "      <th>lda</th>\n",
       "      <th>voting</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>35-49</td>\n",
       "      <td>35-49</td>\n",
       "      <td>35-49</td>\n",
       "      <td>35-49</td>\n",
       "      <td>35-49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>88</td>\n",
       "      <td>56</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       3grams   soac    lda voting   True\n",
       "count      88     88     88     88     88\n",
       "unique      5      4      1      4      5\n",
       "top     35-49  35-49  35-49  35-49  35-49\n",
       "freq       51     52     88     56     37"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas, copy\n",
    "import matplotlib.pyplot as plt\n",
    "pred2 = copy.deepcopy(predictions)\n",
    "pred2.append(y_cv)\n",
    "pred2 = map(list, zip(*pred2))\n",
    "df = pandas.DataFrame(pred2, columns=model_names)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~Bogas/935.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "tls.set_credentials_file(username=\"Bogas\",\n",
    "                             api_key=\"9s60rarm2w\")\n",
    "\n",
    "py.sign_in(username=\"Bogas\", api_key=\"9s60rarm2w\")\n",
    "\n",
    "pred2 = copy.deepcopy(predictions)\n",
    "pred2.append(y_cv)\n",
    "traces = []\n",
    "model_names += ['True']\n",
    "for i, pred in enumerate(pred2):\n",
    "    traces.append(Scatter(\n",
    "        x=range(0,len(y_cv)),\n",
    "        y=pred,\n",
    "        mode='markers+line',\n",
    "        type= 'scatter',\n",
    "        name= model_names[i]\n",
    "        )\n",
    "                )\n",
    "\n",
    "title1 = \"Results on test set for Ensemble Scheme\"\n",
    "layout = Layout(\n",
    "        width= 1200,\n",
    "        height= 800,\n",
    "        title= title1,\n",
    "        xaxis = {\"title\": 'Samples'},\n",
    "        yaxis = {\"title\": 'Classes', \"type\":'category'}\n",
    ")\n",
    "\n",
    "data = Data(traces)\n",
    "fig = Figure(data=data, layout=layout)\n",
    "#py.plot(fig, filename='Grey_70_cosine_vector_list bow')\n",
    "py.iplot(fig, filename=title1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class YAxis in module plotly.graph_objs.graph_objs:\n",
      "\n",
      "class YAxis(PlotlyDict)\n",
      " |  A dictionary-like object for representing a y-axis in plotly.\n",
      " |  \n",
      " |  Online examples:\n",
      " |  \n",
      " |      https://plot.ly/python/axes/\n",
      " |      https://plot.ly/python/multiple-axes/\n",
      " |      https://plot.ly/python/subplots/\n",
      " |      https://plot.ly/python/insets/\n",
      " |  \n",
      " |  Parent key:\n",
      " |  \n",
      " |      yaxis\n",
      " |  \n",
      " |  Quick method reference:\n",
      " |  \n",
      " |      YAxis.update(changes)\n",
      " |      YAxis.strip_style()\n",
      " |      YAxis.get_data()\n",
      " |      YAxis.to_graph_objs()\n",
      " |      YAxis.validate()\n",
      " |      YAxis.to_string()\n",
      " |      YAxis.force_clean()\n",
      " |  \n",
      " |  Valid keys:\n",
      " |  \n",
      " |      title [required=False] (value=a string):\n",
      " |          The y-axis title.\n",
      " |  \n",
      " |      titlefont [required=False] (value=Font object | dictionary-like object):\n",
      " |          Links a dictionary-like object describing the font settings of the\n",
      " |          y-axis title.\n",
      " |  \n",
      " |          For more, run `help(plotly.graph_objs.Font)`\n",
      " |  \n",
      " |      range [required=False] (value=number array of length 2):\n",
      " |          Defines the start and end point of this y-axis.\n",
      " |  \n",
      " |          Examples:\n",
      " |              [-13, 20] | [0, 1]\n",
      " |  \n",
      " |      domain [required=False] (value=number array of length 2):\n",
      " |          Sets the domain of this y-axis; that is, the available space for\n",
      " |          this y-axis to live in. Domain coordinates are given in normalized\n",
      " |          coordinates with respect to the paper.\n",
      " |  \n",
      " |          Examples:\n",
      " |              [0, 0.4] | [0.6, 1]\n",
      " |  \n",
      " |      type [required=False] (value='linear' | 'log' | 'date' | 'category'):\n",
      " |          Sets the format of this axis.\n",
      " |  \n",
      " |      rangemode [required=False] (value='normal' | 'tozero' | 'nonnegative'):\n",
      " |          Choose between Plotly's automated axis generation modes: 'normal'\n",
      " |          (the default) sets the axis range in relation to the extrema in the\n",
      " |          data object, 'tozero' extends the axes to y=0 no matter the data\n",
      " |          plotted and 'nonnegative' sets a non-negative range no matter the\n",
      " |          data plotted.\n",
      " |  \n",
      " |      autorange [required=False] (value=True | False | 'reversed'):\n",
      " |          Toggle whether or not the range of this y-axis is automatically\n",
      " |          picked by Plotly. If 'range' is set, then 'autorange' is set to\n",
      " |          False automatically. Otherwise, if 'autorange' is set to True (the\n",
      " |          default behavior), the range of this y-axis can respond to\n",
      " |          adjustments made in the web GUI automatically. If 'autorange' is set\n",
      " |          to 'reversed', then this y-axis is drawn in reverse, e.g. in a 2D\n",
      " |          plot, from top to bottom instead of from bottom to top (the default\n",
      " |          behavior).\n",
      " |  \n",
      " |      showgrid [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not this axis features grid lines.\n",
      " |  \n",
      " |      zeroline [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not an additional grid line (thicker than the\n",
      " |          other grid lines, by default) will appear on this axis along y=0.\n",
      " |  \n",
      " |      showline [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not the line bounding this y-axis will be shown on\n",
      " |          the figure.\n",
      " |  \n",
      " |      autotick [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not the axis ticks parameters are picked\n",
      " |          automatically by Plotly. Once 'autotick' is set to False, the axis\n",
      " |          ticks parameters can be declared with 'ticks', 'tick0', 'dtick0' and\n",
      " |          other tick-related key in this axis object.\n",
      " |  \n",
      " |      nticks [required=False] (value=number: x > 0):\n",
      " |          Sets the number of axis ticks. No need to set 'autoticks' to False\n",
      " |          for 'nticks' to apply.\n",
      " |  \n",
      " |      ticks [required=False] (value='' | 'inside' | 'outside'):\n",
      " |          Sets the format of the ticks on this axis. For hidden ticks, link\n",
      " |          'ticks' to an empty string.\n",
      " |  \n",
      " |      showticklabels [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not the axis ticks will feature tick labels.\n",
      " |  \n",
      " |      tick0 [required=False] (value=number):\n",
      " |          Sets the starting point of the ticks of this axis.\n",
      " |  \n",
      " |      dtick [required=False] (value=number):\n",
      " |          Sets the distance between ticks on this axis.\n",
      " |  \n",
      " |      ticklen [required=False] (value=number):\n",
      " |          Sets the length of the tick lines on this axis.\n",
      " |  \n",
      " |      tickwidth [required=False] (value=number: x > 0):\n",
      " |          Sets the width of the tick lines on this axis.\n",
      " |  \n",
      " |      tickcolor [required=False] (value=a string describing color):\n",
      " |          Sets the color of the tick lines on this axis.\n",
      " |  \n",
      " |          Examples:\n",
      " |              'green' | 'rgb(0, 255, 0)' | 'rgba(0, 255, 0, 0.3)' |\n",
      " |              'hsl(120,100%,50%)' | 'hsla(120,100%,50%,0.3)' | '#434F1D'\n",
      " |  \n",
      " |      tickangle [required=False] (value=number: x in [-90, 90]):\n",
      " |          Sets the angle in degrees of the ticks on this axis.\n",
      " |  \n",
      " |      tickfont [required=False] (value=Font object | dictionary-like object):\n",
      " |          Links a dictionary-like object defining the parameters of the ticks'\n",
      " |          font.\n",
      " |  \n",
      " |          For more, run `help(plotly.graph_objs.Font)`\n",
      " |  \n",
      " |      exponentformat [required=False] (value='none' | 'e' | 'E' | 'power' |\n",
      " |      'SI' | 'B'):\n",
      " |          Sets how exponents show up. Here's how the number 1000000000 (1\n",
      " |          billion) shows up in each. If set to 'none': 1,000,000,000. If set\n",
      " |          to 'e': 1e+9. If set to 'E': 1E+9. If set to 'power': 1x10^9 (where\n",
      " |          the 9 will appear super-scripted). If set to 'SI': 1G. If set to\n",
      " |          'B': 1B (useful when referring to currency).\n",
      " |  \n",
      " |      showexponent [required=False] (value='all' | 'first' | 'last' | 'none'):\n",
      " |          If set to 'all', ALL exponents will be shown appended to their\n",
      " |          significands. If set to 'first', the first tick's exponent will be\n",
      " |          appended to its significand, however no other exponents will appear\n",
      " |          --only the significands. If set to 'last', the last tick's exponent\n",
      " |          will be appended to its significand, however no other exponents will\n",
      " |          appear--only the significands. If set to 'none', no exponents will\n",
      " |          appear, only the significands.\n",
      " |  \n",
      " |      mirror [required=False] (value=True | False | 'ticks' | 'all' |\n",
      " |      'allticks'):\n",
      " |          Toggle the axis line and/or ticks across the plots or subplots. If\n",
      " |          True, mirror the axis line across the primary subplot (i.e. the axis\n",
      " |          that this axis is anchored to). If 'ticks', mirror the axis line and\n",
      " |          the ticks. If 'all', mirror the axis line to all subplots containing\n",
      " |          this axis. If 'allticks', mirror the line and ticks to all subplots\n",
      " |          containing this axis. If False, don't mirror the axis or the ticks.\n",
      " |  \n",
      " |      gridcolor [required=False] (value=a string describing color):\n",
      " |          Sets the axis grid color.\n",
      " |  \n",
      " |          Examples:\n",
      " |              'green' | 'rgb(0, 255, 0)' | 'rgba(0, 255, 0, 0.3)' |\n",
      " |              'hsl(120,100%,50%)' | 'hsla(120,100%,50%,0.3)' | '#434F1D'\n",
      " |  \n",
      " |      gridwidth [required=False] (value=number: x > 0):\n",
      " |          Sets the grid width (in pixels).\n",
      " |  \n",
      " |      zerolinecolor [required=False] (value=a string describing color):\n",
      " |          Sets the color of this axis' zeroline.\n",
      " |  \n",
      " |          Examples:\n",
      " |              'green' | 'rgb(0, 255, 0)' | 'rgba(0, 255, 0, 0.3)' |\n",
      " |              'hsl(120,100%,50%)' | 'hsla(120,100%,50%,0.3)' | '#434F1D'\n",
      " |  \n",
      " |      zerolinewidth [required=False] (value=number: x > 0):\n",
      " |          Sets the width of this axis' zeroline (in pixels).\n",
      " |  \n",
      " |      linecolor [required=False] (value=a string describing color):\n",
      " |          Sets the axis line color.\n",
      " |  \n",
      " |          Examples:\n",
      " |              'green' | 'rgb(0, 255, 0)' | 'rgba(0, 255, 0, 0.3)' |\n",
      " |              'hsl(120,100%,50%)' | 'hsla(120,100%,50%,0.3)' | '#434F1D'\n",
      " |  \n",
      " |      linewidth [required=False] (value=number: x > 0):\n",
      " |          Sets the width of the axis line (in pixels).\n",
      " |  \n",
      " |      anchor [required=False] (value='x' | 'x1' | 'x2' | ... | 'free' ):\n",
      " |          Choose whether the position of this y-axis will be anchored to a\n",
      " |          corresponding x-axis or will be 'free' to appear anywhere in the\n",
      " |          horizontal space of this figure. Has no effect in 3D plots.\n",
      " |  \n",
      " |      overlaying [required=False] (value='y' | 'y1' | 'y2' | ... | False):\n",
      " |          Choose to overlay the data bound to this y-axis on the same plotting\n",
      " |          area as a corresponding x-axis or choose not overlay other y-the\n",
      " |          other axis/axes of this figure.Has no effect in 3D plots.\n",
      " |  \n",
      " |      side [required=False] (value='left' | 'right'):\n",
      " |          Sets whether this y-axis sits at the 'left' of the plot or at the\n",
      " |          'right' of the plot.Has no effect in 3D plots.\n",
      " |  \n",
      " |      position [required=False] (value=number: x in [0, 1]):\n",
      " |          Sets where this y-axis is positioned in the plotting space. For\n",
      " |          example if 'position' is set to 0.5, then this axis is placed at the\n",
      " |          exact center of the plotting space. Has an effect only if 'anchor'\n",
      " |          is set to 'free'.Has no effect in 3D plots.\n",
      " |  \n",
      " |      showbackground [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not this y-axis will have a background color. Has\n",
      " |          an effect only in 3D plots.\n",
      " |  \n",
      " |      backgroundcolor [required=False] (value=a string describing color):\n",
      " |          Sets the background color of this y-axis. Has an effect only in 3D\n",
      " |          plots and if 'showbackground' is set to True.\n",
      " |  \n",
      " |      showspikes [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not spikes will link up to this y-axis when\n",
      " |          hovering over data points. Has an effect only in 3D plots.\n",
      " |  \n",
      " |      spikesides [required=False] (value=a boolean: True | False):\n",
      " |          Toggle whether or not the spikes will expand out to the y-axis\n",
      " |          bounds when hovering over data points. Has an effect only in 3D\n",
      " |          plots and if 'showspikes' is set to True.\n",
      " |  \n",
      " |      spikethickness [required=False] (value=number: x > 0):\n",
      " |          Sets the thickness (in pixels) of the y-axis spikes.Has an effect\n",
      " |          only in 3D plots and if 'showspikes' is set to True.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      YAxis\n",
      " |      PlotlyDict\n",
      " |      __builtin__.dict\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods inherited from PlotlyDict:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  force_clean(self, caller=True)\n",
      " |      Attempts to convert to graph_objs and call force_clean() on values.\n",
      " |      \n",
      " |      Calling force_clean() on a PlotlyDict will ensure that the object is\n",
      " |      valid and may be sent to plotly. This process will also remove any\n",
      " |      entries that end up with a length == 0.\n",
      " |      \n",
      " |      Careful! This will delete any invalid entries *silently*.\n",
      " |  \n",
      " |  get_data(self, flatten=False)\n",
      " |      Returns the JSON for the plot with non-data elements stripped.\n",
      " |  \n",
      " |  get_ordered(self, caller=True)\n",
      " |  \n",
      " |  strip_style(self)\n",
      " |      Strip style from the current representation.\n",
      " |      \n",
      " |      All PlotlyDicts and PlotlyLists are guaranteed to survive the\n",
      " |      stripping process, though they made be left empty. This is allowable.\n",
      " |      \n",
      " |      Keys that will be stripped in this process are tagged with\n",
      " |      `'type': 'style'` in graph_objs_meta.json.\n",
      " |      \n",
      " |      This process first attempts to convert nested collections from dicts\n",
      " |      or lists to subclasses of PlotlyList/PlotlyDict. This process forces\n",
      " |      a validation, which may throw exceptions.\n",
      " |      \n",
      " |      Then, each of these objects call `strip_style` on themselves and so\n",
      " |      on, recursively until the entire structure has been validated and\n",
      " |      stripped.\n",
      " |  \n",
      " |  to_graph_objs(self, caller=True)\n",
      " |      Walk obj, convert dicts and lists to plotly graph objs.\n",
      " |      \n",
      " |      For each key in the object, if it corresponds to a special key that\n",
      " |      should be associated with a graph object, the ordinary dict or list\n",
      " |      will be reinitialized as a special PlotlyDict or PlotlyList of the\n",
      " |      appropriate `kind`.\n",
      " |  \n",
      " |  to_string(self, level=0, indent=4, eol='\\n', pretty=True, max_chars=80)\n",
      " |      Returns a formatted string showing graph_obj constructors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          print(obj.to_string())\n",
      " |      \n",
      " |      Keyword arguments:\n",
      " |      level (default = 0) -- set number of indentations to start with\n",
      " |      indent (default = 4) -- set indentation amount\n",
      " |      eol (default = '\\n') -- set end of line character(s)\n",
      " |      pretty (default = True) -- curtail long list output with a '...'\n",
      " |      max_chars (default = 80) -- set max characters per line\n",
      " |  \n",
      " |  update(self, dict1=None, **dict2)\n",
      " |      Update current dict with dict1 and then dict2.\n",
      " |      \n",
      " |      This recursively updates the structure of the original dictionary-like\n",
      " |      object with the new entries in the second and third objects. This\n",
      " |      allows users to update with large, nested structures.\n",
      " |      \n",
      " |      Note, because the dict2 packs up all the keyword arguments, you can\n",
      " |      specify the changes as a list of keyword agruments.\n",
      " |      \n",
      " |      Examples:\n",
      " |      # update with dict\n",
      " |      obj = Layout(title='my title', xaxis=XAxis(range=[0,1], domain=[0,1]))\n",
      " |      update_dict = dict(title='new title', xaxis=dict(domain=[0,.8]))\n",
      " |      obj.update(update_dict)\n",
      " |      obj\n",
      " |      {'title': 'new title', 'xaxis': {'range': [0,1], 'domain': [0,.8]}}\n",
      " |      \n",
      " |      # update with list of keyword arguments\n",
      " |      obj = Layout(title='my title', xaxis=XAxis(range=[0,1], domain=[0,1]))\n",
      " |      obj.update(title='new title', xaxis=dict(domain=[0,.8]))\n",
      " |      obj\n",
      " |      {'title': 'new title', 'xaxis': {'range': [0,1], 'domain': [0,.8]}}\n",
      " |      \n",
      " |      This 'fully' supports duck-typing in that the call signature is\n",
      " |      identical, however this differs slightly from the normal update\n",
      " |      method provided by Python's dictionaries.\n",
      " |  \n",
      " |  validate(self, caller=True)\n",
      " |      Recursively check the validity of the keys in a PlotlyDict.\n",
      " |      \n",
      " |      The valid keys constitute the entries in each object\n",
      " |      dictionary in graph_objs_meta.json\n",
      " |      \n",
      " |      The validation process first requires that all nested collections be\n",
      " |      converted to the appropriate subclass of PlotlyDict/PlotlyList. Then,\n",
      " |      each of these objects call `validate` and so on, recursively,\n",
      " |      until the entire object has been validated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from PlotlyDict:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from __builtin__.dict:\n",
      " |  \n",
      " |  __cmp__(...)\n",
      " |      x.__cmp__(y) <==> cmp(x,y)\n",
      " |  \n",
      " |  __contains__(...)\n",
      " |      D.__contains__(k) -> True if D has a key k, else False\n",
      " |  \n",
      " |  __delitem__(...)\n",
      " |      x.__delitem__(y) <==> del x[y]\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      x.__eq__(y) <==> x==y\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      x.__ge__(y) <==> x>=y\n",
      " |  \n",
      " |  __getattribute__(...)\n",
      " |      x.__getattribute__('name') <==> x.name\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      x.__gt__(y) <==> x>y\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      x.__iter__() <==> iter(x)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      x.__le__(y) <==> x<=y\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      x.__len__() <==> len(x)\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      x.__lt__(y) <==> x<y\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      x.__ne__(y) <==> x!=y\n",
      " |  \n",
      " |  __repr__(...)\n",
      " |      x.__repr__() <==> repr(x)\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  fromkeys(...)\n",
      " |      dict.fromkeys(S[,v]) -> New dict with keys from S and values equal to v.\n",
      " |      v defaults to None.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  has_key(...)\n",
      " |      D.has_key(k) -> True if D has a key k, else False\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> list of D's (key, value) pairs, as 2-tuples\n",
      " |  \n",
      " |  iteritems(...)\n",
      " |      D.iteritems() -> an iterator over the (key, value) items of D\n",
      " |  \n",
      " |  iterkeys(...)\n",
      " |      D.iterkeys() -> an iterator over the keys of D\n",
      " |  \n",
      " |  itervalues(...)\n",
      " |      D.itervalues() -> an iterator over the values of D\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> list of D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> list of D's values\n",
      " |  \n",
      " |  viewitems(...)\n",
      " |      D.viewitems() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  viewkeys(...)\n",
      " |      D.viewkeys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  viewvalues(...)\n",
      " |      D.viewvalues() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from __builtin__.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __new__ = <built-in method __new__ of type object>\n",
      " |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(YAxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are transforming!\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# predict class probabilities for all classifiers\n",
    "probas = [c.predict_proba(X_cv) for c in trained_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "import numpy as np\n",
    "\n",
    "y_cv2 = label_binarize(y_cv, list(set(y)))\n",
    "pred2 = []\n",
    "for pred in predictions:\n",
    "    pred2.append(label_binarize(pred, list(set(y))))\n",
    "\n",
    "n_classes = len(list(set(y)))\n",
    "plt.figure()    \n",
    "for j, model in enumerate(trained_models):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(list(set(y)))):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_cv2[:, i], pred2[j][:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label=model_names[j]+' macro-area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics to multi-class for ensemble methods')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LDAA = grid_search.best_estimator_.steps[0][1].__dict__['transformer_list'][0][1]\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_words = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(\"#%d: \" % topic_idx + topic_words)\n",
    "        #print(\" \".join([feature_names[i]\n",
    "        #                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "#print_top_words(LDAA.LDA, LDAA.counter.get_feature_names(), 10)\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "     \n",
    "    feat = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_words = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        feat.append(\"#%d: \" % topic_idx + topic_words)\n",
    "        #print(\"#%d: \" % topic_idx + topic_words)\n",
    "    return feat\n",
    "get_top_words(LDAA.LDA, LDAA.counter.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = get_top_words(LDAA.LDA, LDAA.counter.get_feature_names(), 10)\n",
    "print len(feature_names)\n",
    "#soa_feat_names = [\"soa_prob_\"+str(i) for i in range(0, len(set(y)))]\n",
    "#soac_feat_names = [\"soac_prob_\"+str(i) for i in range(0, len(set(y)))]\n",
    "#feature_names += soa_feat_names\n",
    "#feature_names += soac_feat_names\n",
    "feature_names = [feat.encode('utf-8') for feat in feature_names]\n",
    "print len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts + soa + Soac Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "feature_names = copy.deepcopy(countTokens.l)\n",
    "feature_names += ['numHash', 'numUrl', 'numRep']\n",
    "#soa_feat_names = [\"soa_prob_\"+str(i) for i in range(0, len(set(y)))]\n",
    "#soac_feat_names = [\"soac_prob_\"+str(i) for i in range(0, len(set(y)))]\n",
    "#feature_names += soa_feat_names\n",
    "#feature_names += soac_feat_names\n",
    "feature_names = [feat.encode('utf-8') for feat in feature_names]\n",
    "print len(countTokens.l), len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(features)\n",
    "#features.SOAC_Model2.__doc__\n",
    "soac = features.SOAC_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=5000)\n",
    "#y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX = [#\"I like playing video games very much :).\", \n",
    "     #\"Football games are the best!\",\n",
    "     #\"Being young forever is very funny and entertaining\",\n",
    "     # \"Football games are the best!\",\n",
    "      \"best games\",\n",
    "      \"best games\",\n",
    "     #\"World leaders should gather and decide for todays meeting!\",\n",
    "     #\"Problems nowadays seem to thrive everywhere\",\n",
    "     #\"Just got off from work today! Weekend is coming though, so it's alright...\",\n",
    "     #\"This weekend we are going of for 3 days..\",\n",
    "     \" Weekend alright...\",\n",
    "     \" Weekend alright...\",\n",
    "     \" Weekend alright...\",\n",
    "     \"Awful weather\",\n",
    "     \"Awful weather\",\n",
    "     \"Awful weather\",\n",
    "     \"Awful weather\",\n",
    "     \"Awful weather\"]\n",
    "yy = [\"18-24\",\n",
    "     \"18-24\",\n",
    "     \"25-34\",\n",
    "     \"25-34\",\n",
    "     \"25-34\",\n",
    "     \"35-49\",\n",
    "     \"35-49\",\n",
    "     \"35-49\",\n",
    "     \"35-49\",\n",
    "     \"35-49\",\n",
    "    ]\n",
    "#reload(preprocess)\n",
    "#reload(features)\n",
    "from pan import features\n",
    "from pan import preprocess\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "XX = preprocess.preprocess(XX)\n",
    "num_folds = 2\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "grid_search.fit(XX,yy)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = soac.fit_transform(X[0:10], y[0:10])\n",
    "print xx\n",
    "print y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('combined', FeatureUnion(n_jobs=1,\n",
       "         transformer_list=[('soac', SOAC_Model2(max_df=1.0, max_features=None, min_df=1, thres=0.1,\n",
       "        tokenizer_var='sklearn'))],\n",
       "         transformer_weights=None)),\n",
       " ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soac = features.SOAC_Model2(max_df=1.0, min_df=1, tokenizer_var='sklearn', max_features=None)\n",
    "combined = FeatureUnion([('soac', soac)])\n",
    "svm = SVC(kernel='rbf', C=0.1, gamma=1, class_weight='balanced', probability=True)\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies), \n",
    "#                          ('soa', soa), ('soac', soac)])\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "pipe1 = Pipeline([('combined',combined), ('svm', svm)])\n",
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('combined', FeatureUnion(n_jobs=1,\n",
       "         transformer_list=[('soa', SOA_Model2(max_df=1.0, max_features=None, min_df=5, tokenizer_var='sklearn'))],\n",
       "         transformer_weights=None)),\n",
       " ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', C=0.1, gamma=1, class_weight='balanced', probability=True)\n",
    "soa = features.SOA_Model2(max_df=1.0, min_df=5, tokenizer_var='sklearn', max_features=None)\n",
    "combined = FeatureUnion([('soa', soa)])\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies), \n",
    "#                          ('soa', soa), ('soac', soac)])\n",
    "#combined = FeatureUnion([('count_tokens', countTokens), ('count_hash', countHash),\n",
    "#                         ('count_urls', countUrls), ('count_replies', countReplies)])\n",
    "pipe2= Pipeline([('combined',combined), ('svm', svm)])\n",
    "pipe2.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "num_folds = 4\n",
    "grid_search1 = GridSearchCV(estimator=pipe1, param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "grid_search1.fit(X,y)\n",
    "print(grid_search1.best_estimator_)\n",
    "print(grid_search1.best_score_)\n",
    "grid_search2 = GridSearchCV(estimator=pipe2, param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "grid_search2.fit(X,y)\n",
    "print(grid_search2.best_estimator_)\n",
    "print(grid_search2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "We are fitting!\n",
      "[[ 15.57142857   3.11428571   2.3956044    5.45        72.66666667]]We are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "[[ 15.52380952   3.1047619    2.39705882   5.43333333  81.5       ]][[ 15.52380952   3.1047619    2.39705882   5.43333333  81.5       ]][[ 15.61904762   3.12380952   2.39416058   5.46666667  65.6       ]][[ 15.61904762   3.12380952   2.39416058   5.46666667  65.6       ]]\n",
      "\n",
      "\n",
      "\n",
      "We are transforming!We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "\n",
      "Doc_profDoc_profDoc_profDoc_prof\n",
      "\n",
      "\n",
      "\n",
      "(326, 5) <type 'numpy.ndarray'>\n",
      "(328, 5) <type 'numpy.ndarray'>\n",
      "(328, 5) <type 'numpy.ndarray'>\n",
      "(326, 5) <type 'numpy.ndarray'>\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_profDoc_profDoc_profDoc_prof\n",
      "\n",
      "\n",
      "\n",
      "(110, 5) <type 'numpy.ndarray'>\n",
      "(108, 5) <type 'numpy.ndarray'>\n",
      "(108, 5) <type 'numpy.ndarray'>\n",
      "(110, 5) <type 'numpy.ndarray'>\n",
      "\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(436, 5) <type 'numpy.ndarray'>\n",
      "Pipeline(steps=[('combined', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('soac', SOAC_Model2(max_df=1.0, max_features=None, min_df=1, thres=0.1,\n",
      "      tokenizer_var='sklearn'))],\n",
      "       transformer_weights=None)), ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "0.447247706422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "num_folds = 4\n",
    "grid_search = GridSearchCV(estimator=pipe1, param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "grid_search.fit(X,y)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 88 436 436\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "We are fitting!\n",
      "Doc_TermsWe are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "Doc_TermsDoc_TermsDoc_TermsDoc_Terms\n",
      "\n",
      "\n",
      "\n",
      "(259, 13537)\n",
      "(261, 14138)\n",
      "(262, 14058)\n",
      "(262, 13356)\n",
      "Doc_Prof\n",
      "Doc_Prof\n",
      "Doc_Prof\n",
      "Doc_Prof\n",
      "(259, 5) <type 'numpy.ndarray'>\n",
      "(261, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "Doc_Term\n",
      "Doc_Term\n",
      "Doc_Term\n",
      "Doc_Term\n",
      "(259, 13537) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(261, 14138) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 14058) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 13356) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Term_Prof\n",
      "Term_Prof\n",
      "Term_Prof\n",
      "Term_Prof\n",
      "(13537, 5) <type 'numpy.ndarray'>\n",
      "(14138, 5) <type 'numpy.ndarray'>\n",
      "(14058, 5) <type 'numpy.ndarray'>\n",
      "(13356, 5) <type 'numpy.ndarray'>\n",
      "Random Term_Prof\n",
      "Random Term_Prof\n",
      "Random Term_Prof\n",
      "Random Term_Prof\n",
      "[ 0.          0.44151325  0.06550793  0.49297881  0.        ]\n",
      "[ 0.          0.72245332  0.27754668  0.          0.        ]\n",
      "[ 0.          0.34643333  0.17271636  0.48085031  0.        ]\n",
      "[ 0.          0.30669112  0.21100297  0.48230591  0.        ]\n",
      "SOA Model Fitted!\n",
      "SOA Model Fitted!\n",
      "SOA Model Fitted!\n",
      "SOA Model Fitted!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_TermsDoc_TermsDoc_TermsDoc_Terms\n",
      "\n",
      "\n",
      "\n",
      "(259, 13537) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(261, 14138) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 14058) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 13356) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "(259, 5) <type 'numpy.ndarray'>\n",
      "(261, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "[ 2.8290876   2.75209922  3.23799907  2.85175416  1.42228738]\n",
      "[ 1.17531373  1.56845209  2.38956113  1.86018563  1.08943037]\n",
      "[ 1.22597844  1.58720898  2.48222065  1.77605125  1.03818736]\n",
      "[ 1.1048924   1.63160465  2.38379453  1.85462345  1.10881593]\n",
      "Len Voc: 13537\n",
      "Len Voc: 14138\n",
      "Len Voc: 14058\n",
      "Len Voc: 13356\n",
      "We are transforming!We are transforming!We are transforming!We are transforming!\n",
      "\n",
      "\n",
      "\n",
      "Doc_TermsDoc_TermsDoc_TermsDoc_Terms\n",
      "\n",
      "\n",
      "\n",
      "(89, 13537) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(87, 14138) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(86, 14058) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(86, 13356) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "(89, 5) <type 'numpy.ndarray'>\n",
      "(87, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "[ 1.26691404  1.66449085  2.30417331  1.78089942  1.03220281]\n",
      "[ 2.37359272  2.92592043  2.90394709  3.1043561   1.68510674]\n",
      "[ 1.59815326  1.92811313  2.18654839  2.30936014  1.40540878]\n",
      "[ 2.26109734  2.51535375  2.34143872  2.25845794  1.45798678]\n",
      "Len Voc: 13537\n",
      "Len Voc: 14138\n",
      "Len Voc: 14058\n",
      "Len Voc: 13356\n",
      "\n",
      "(348, 16756)\n",
      "Doc_Prof\n",
      "(348, 5) <type 'numpy.ndarray'>\n",
      "Doc_Term\n",
      "(348, 16756) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Term_Prof\n",
      "(16756, 5) <type 'numpy.ndarray'>\n",
      "Random Term_Prof\n",
      "[ 0.          0.43153198  0.17457671  0.39389131  0.        ]\n",
      "SOA Model Fitted!\n",
      "We are transforming!\n",
      "Doc_Terms\n",
      "(348, 16756) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "(348, 5) <type 'numpy.ndarray'>\n",
      "[ 1.20380878  1.61260998  2.39780476  1.82296852  1.11357178]\n",
      "Len Voc: 16756\n",
      "0.456896551724\n",
      "Pipeline(steps=[('combined', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('3grams', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngra...',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are fitting!\n",
      "[[ 15.81818182   3.10714286   2.4          5.4375      69.6       ]]We are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "[[ 16.1875       3.08333333   2.39814815   5.39583333  86.33333333]][[ 16.3125       3.10714286   2.39449541   5.4375      65.25      ]][[ 15.41176471   3.11904762   2.40366972   5.45833333  65.5       ]][[ 15.41176471   3.11904762   2.40366972   5.45833333  65.5       ]]\n",
      "\n",
      "\n",
      "\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_profDoc_profDoc_profDoc_prof\n",
      "\n",
      "\n",
      "\n",
      "(259, 5) <type 'numpy.ndarray'>\n",
      "(261, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_profDoc_profDoc_profDoc_prof\n",
      "\n",
      "\n",
      "\n",
      "(89, 5) <type 'numpy.ndarray'>\n",
      "(87, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(348, 5) <type 'numpy.ndarray'>\n",
      "0.454022988506\n",
      "Pipeline(steps=[('combined', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('soac', SOAC_Model2(max_df=1.0, max_features=None, min_df=1, tokenizer_var='sklearn'))],\n",
      "       transformer_weights=None)), ('svm', SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are fitting!\n",
      "Doc_TermsWe are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "We are fitting!\n",
      "Doc_TermsDoc_TermsDoc_TermsDoc_Terms\n",
      "\n",
      "\n",
      "\n",
      "(259, 13537)\n",
      "(261, 14138)\n",
      "(262, 14058)\n",
      "(262, 13356)\n",
      "Doc_Prof\n",
      "Doc_Prof\n",
      "Doc_Prof\n",
      "Doc_Prof\n",
      "(259, 5) <type 'numpy.ndarray'>\n",
      "(261, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "Doc_Term\n",
      "Doc_Term\n",
      "Doc_Term\n",
      "Doc_Term\n",
      "(259, 13537) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(261, 14138) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 14058) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 13356) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Term_Prof\n",
      "Term_Prof\n",
      "Term_Prof\n",
      "Term_Prof\n",
      "(13537, 5) <type 'numpy.ndarray'>\n",
      "(14138, 5) <type 'numpy.ndarray'>\n",
      "(14058, 5) <type 'numpy.ndarray'>\n",
      "(13356, 5) <type 'numpy.ndarray'>\n",
      "Random Term_Prof\n",
      "Random Term_Prof\n",
      "Random Term_Prof\n",
      "Random Term_Prof\n",
      "[ 0.          0.44151325  0.06550793  0.49297881  0.        ]\n",
      "[ 0.          0.72245332  0.27754668  0.          0.        ]\n",
      "[ 0.          0.34643333  0.17271636  0.48085031  0.        ]\n",
      "[ 0.          0.30669112  0.21100297  0.48230591  0.        ]\n",
      "SOA Model Fitted!\n",
      "SOA Model Fitted!\n",
      "SOA Model Fitted!\n",
      "SOA Model Fitted!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_TermsDoc_TermsDoc_TermsDoc_Terms\n",
      "\n",
      "\n",
      "\n",
      "(259, 13537) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(261, 14138) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 14058) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(262, 13356) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "(259, 5) <type 'numpy.ndarray'>\n",
      "(261, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "[ 2.8290876   2.75209922  3.23799907  2.85175416  1.42228738]\n",
      "[ 1.17531373  1.56845209  2.38956113  1.86018563  1.08943037]\n",
      "[ 1.22597844  1.58720898  2.48222065  1.77605125  1.03818736]\n",
      "[ 1.1048924   1.63160465  2.38379453  1.85462345  1.10881593]\n",
      "Len Voc: 13537\n",
      "Len Voc: 14138\n",
      "Len Voc: 14058\n",
      "Len Voc: 13356\n",
      "We are fitting!We are fitting!We are fitting!We are fitting!\n",
      "\n",
      "\n",
      "\n",
      "[[ 16.1875       3.08333333   2.39814815   5.39583333  86.33333333]][[ 16.3125       3.10714286   2.39449541   5.4375      65.25      ]][[ 15.41176471   3.11904762   2.40366972   5.45833333  65.5       ]][[ 15.41176471   3.11904762   2.40366972   5.45833333  65.5       ]]\n",
      "\n",
      "\n",
      "\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_profDoc_profDoc_profDoc_prof\n",
      "\n",
      "\n",
      "\n",
      "(259, 5) <type 'numpy.ndarray'>\n",
      "(261, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "(262, 5) <type 'numpy.ndarray'>\n",
      "We are transforming!We are transforming!We are transforming!We are transforming!\n",
      "\n",
      "\n",
      "\n",
      "Doc_TermsDoc_TermsDoc_TermsDoc_Terms\n",
      "\n",
      "\n",
      "\n",
      "(89, 13537) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(87, 14138) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(86, 14058) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(86, 13356) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "Doc_prof\n",
      "(89, 5) <type 'numpy.ndarray'>\n",
      "(87, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "[ 1.26691404  1.66449085  2.30417331  1.78089942  1.03220281]\n",
      "[ 2.37359272  2.92592043  2.90394709  3.1043561   1.68510674]\n",
      "[ 1.59815326  1.92811313  2.18654839  2.30936014  1.40540878]\n",
      "[ 2.26109734  2.51535375  2.34143872  2.25845794  1.45798678]\n",
      "Len Voc: 13537\n",
      "Len Voc: 14138\n",
      "Len Voc: 14058\n",
      "Len Voc: 13356\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "We are transforming!\n",
      "Doc_profDoc_profDoc_profDoc_prof\n",
      "\n",
      "\n",
      "\n",
      "(89, 5) <type 'numpy.ndarray'>\n",
      "(87, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "(86, 5) <type 'numpy.ndarray'>\n",
      "\n",
      "(348, 16756)\n",
      "Doc_Prof\n",
      "(348, 5) <type 'numpy.ndarray'>\n",
      "Doc_Term\n",
      "(348, 16756) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Term_Prof\n",
      "(16756, 5) <type 'numpy.ndarray'>\n",
      "Random Term_Prof\n",
      "[ 0.          0.43153198  0.17457671  0.39389131  0.        ]\n",
      "SOA Model Fitted!\n",
      "We are transforming!\n",
      "Doc_Terms\n",
      "(348, 16756) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "(348, 5) <type 'numpy.ndarray'>\n",
      "[ 1.20380878  1.61260998  2.39780476  1.82296852  1.11357178]\n",
      "Len Voc: 16756\n",
      "We are fitting!\n",
      "[[ 15.81818182   3.10714286   2.4          5.4375      69.6       ]]\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(348, 5) <type 'numpy.ndarray'>\n",
      "0.471264367816\n",
      "VotingClassifier(estimators=[('0', Pipeline(steps=[('combined', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('3grams', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=5...  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))]))],\n",
      "         voting='soft', weights=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "num_folds = 4\n",
    "split = 0.2\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, random_state=42, stratify=y)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "eclf = VotingClassifier(estimators=[(\"0\", pipe), ('1', pipe1)], voting='soft')\n",
    "trained_models = []\n",
    "for model in [pipe, pipe1, eclf]:\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid={}, verbose=1, n_jobs=-1, cv=num_folds, refit=True)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_estimator_) \n",
    "    trained_models.append(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are transforming!\n",
      "Doc_Terms\n",
      "(88, 16756) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n",
      "[ 3.1563402   3.32837876  3.16486118  2.96139769  1.89792746]\n",
      "Len Voc: 16756\n",
      "Accuracy : 0.534090909091\n",
      "Confusion matrix :\n",
      " [[15 12  1  0  0]\n",
      " [ 6 29  2  0  0]\n",
      " [ 4 10  2  0  0]\n",
      " [ 3  2  0  1  0]\n",
      " [ 0  1  0  0  0]]\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n",
      "Accuracy : 0.511363636364\n",
      "Confusion matrix :\n",
      " [[ 9 17  0  2  0]\n",
      " [ 2 33  0  2  0]\n",
      " [ 1 14  1  0  0]\n",
      " [ 2  2  0  2  0]\n",
      " [ 0  1  0  0  0]]\n",
      "We are transforming!\n",
      "Doc_Terms\n",
      "(88, 16756) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SOA Transform:\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n",
      "[ 3.1563402   3.32837876  3.16486118  2.96139769  1.89792746]\n",
      "Len Voc: 16756\n",
      "We are transforming!\n",
      "Doc_prof\n",
      "(88, 5) <type 'numpy.ndarray'>\n",
      "Accuracy : 0.568181818182\n",
      "Confusion matrix :\n",
      " [[15 12  1  0  0]\n",
      " [ 5 32  0  0  0]\n",
      " [ 5  9  2  0  0]\n",
      " [ 3  2  0  1  0]\n",
      " [ 0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for model in trained_models:\n",
    "    predict = model.predict(X_cv)\n",
    "    predictions.append(predict)\n",
    "    acc = accuracy_score(y_cv, predict)\n",
    "    conf = confusion_matrix(y_cv, predict, labels=list(set(y)))\n",
    "    print('Accuracy : {}'.format(acc))\n",
    "    print('Confusion matrix :\\n {}'.format(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['35-49', '25-34', '25-34', '25-34', '35-49', '50-64', '35-49',\n",
       "       '35-49', '35-49', '25-34', '35-49', '25-34', '50-64', '35-49',\n",
       "       '35-49', '25-34', '25-34', '35-49', '35-49', '35-49', '25-34',\n",
       "       '25-34', '35-49', '25-34', '25-34', '35-49', '35-49', '35-49',\n",
       "       '35-49', '35-49', '35-49', '25-34', '35-49', '25-34', '25-34',\n",
       "       '25-34', '35-49', '25-34', '35-49', '35-49', '25-34', '25-34',\n",
       "       '35-49', '35-49', '25-34', '25-34', '35-49', '35-49', '35-49',\n",
       "       '25-34', '50-64', '25-34', '35-49', '25-34', '35-49', '25-34',\n",
       "       '25-34', '25-34', '25-34', '35-49', '35-49', '25-34', '25-34',\n",
       "       '18-24', '35-49', '35-49', '25-34', '35-49', '50-64', '25-34',\n",
       "       '35-49', '35-49', '25-34', '50-64', '35-49', '35-49', '35-49',\n",
       "       '35-49', '18-24', '35-49', '35-49', '35-49', '25-34', '25-34',\n",
       "       '25-34', '18-24', '50-64', '50-64', '35-49', '25-34', '35-49',\n",
       "       '35-49', '25-34', '25-34', '35-49', '35-49', '25-34', '25-34',\n",
       "       '25-34', '25-34', '25-34', '35-49', '35-49', '25-34', '35-49',\n",
       "       '35-49', '25-34', '18-24', '35-49', '35-49', '25-34', '35-49',\n",
       "       '50-64', '35-49', '35-49', '18-24', '25-34', '35-49', '35-49',\n",
       "       '25-34', '35-49', '18-24', '35-49', '25-34', '35-49', '18-24',\n",
       "       '35-49', '35-49', '25-34', '25-34', '35-49'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3grams + soa + Soac Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#feature_names = grid_search.best_estimator_.steps[0][1].__dict__['transformer_list'][0][1].get_feature_names()\n",
    "#print len(set(y))\n",
    "feature_names = []\n",
    "soa_feat_names = [\"soa_prob_\"+str(i) for i in range(0, len(set(y)))]\n",
    "soac_feat_names = [\"soac_prob_\"+str(i) for i in range(0, len(set(y)))]\n",
    "#feature_names += soa_feat_names\n",
    "feature_names += soac_feat_names\n",
    "print len(feature_names)\n",
    "feature_names = [feat.encode('utf-8') for feat in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = grid_search.best_estimator_.steps[0][1]\n",
    "#print a.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are transforming!\n",
      "Doc_prof\n",
      "(436, 5) <type 'numpy.ndarray'>\n",
      "       soac_prob_0  soac_prob_1  soac_prob_2  soac_prob_3  soac_prob_4\n",
      "count   436.000000   436.000000   436.000000   436.000000   436.000000\n",
      "mean      0.334800     0.333926     0.323147     0.335459     0.333121\n",
      "std       0.245069     0.370890     0.390482     0.307089     0.231141\n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%       0.189950     0.000000     0.000000     0.141166     0.190086\n",
      "50%       0.289293     0.261512     0.263432     0.289500     0.284627\n",
      "75%       0.428288     0.512928     0.523927     0.448032     0.394424\n",
      "max       1.692613     2.068010     3.497002     2.356757     1.905468\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(a.transform(X), columns=feature_names)\n",
    "data[\"class\"] = y\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soacc = a.transformer_list[0][1]\n",
    "voc = soacc.counter.vocabulary_\n",
    "print 'Voc: ' + str(len(voc))\n",
    "print soacc.term_table.shape\n",
    "#terms= ['marriage', 'pension']\n",
    "#graph_matrix = numpy.zeros([len(terms), soacc.term_table.shape[1]])\n",
    "j = 0 \n",
    "for term, index in voc.iteritems():\n",
    "    l = list(soacc.term_table[index,:])\n",
    "    if l.index(min(l))==3 and  min(l)<0.02 and min(l)!=0:\n",
    "        print term\n",
    "        print l\n",
    "        j += 1\n",
    "    if j==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voc: 125532\n",
      "(125532, 5)\n",
      "dreamjob\n",
      "[ 0.01265381  0.26222892  0.31307008  0.22406249  0.18798469]\n",
      "lol\n",
      "[ 0.19276541  0.11180198  0.27522844  0.2184271   0.20177707]\n",
      "mortgage\n",
      "[ 0.2011231   0.2456243   0.14738652  0.20176227  0.2041038 ]\n",
      "booksellers\n",
      "[ 0.20404603  0.2865569   0.3084714   0.00853856  0.1923871 ]\n",
      "juvenile\n",
      "[ 0.19876243  0.27913675  0.27862587  0.22337202  0.02010292]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~Bogas/929.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy\n",
    "py.sign_in('Bogas', '9s60rarm2w')\n",
    "soacc = a.transformer_list[0][1]\n",
    "voc = soacc.counter.vocabulary_\n",
    "print 'Voc: ' + str(len(voc))\n",
    "print soacc.term_table.shape\n",
    "terms= ['dreamjob','lol', 'mortgage', 'booksellers', 'juvenile']\n",
    "graph_matrix = numpy.zeros([len(terms), soacc.term_table.shape[1]])\n",
    "j = 0\n",
    "for term in terms:\n",
    "    idx = voc[term]\n",
    "    print term\n",
    "    print soacc.term_table[idx,:]\n",
    "    graph_matrix[j, :] = soacc.term_table[idx,:]\n",
    "    j += 1\n",
    "    #plt.bar(numpy.arange(soacc.term_table.shape[1]), soacc.term_table[idx,:], color='r')\n",
    "    #plt.show()\n",
    "\n",
    "data = []\n",
    "names = sorted(list(set(y)))\n",
    "for i in range(0, soacc.term_table.shape[1]):\n",
    "    data.append(\n",
    "        go.Bar(\n",
    "        x=terms,\n",
    "        y=graph_matrix[:, i],\n",
    "        name=names[i]\n",
    "    )\n",
    "    )\n",
    "layout = go.Layout(\n",
    "    barmode='group'\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)\n",
    "#plot_url = py.plot(fig, filename='grouped-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('25-34', <matplotlib.axes._subplots.AxesSubplot object at 0x7f62e40e63d0>), ('35-49', <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6f0c290>), ('50-64', <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6eac590>), ('18-24', <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6ebddd0>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6e33390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6e05c10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6d8c310>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6d02290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6cd9790>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6c4f810>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6bad110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6b26350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6bd6990>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f62c6a799d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c69f4950>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62c69cfe50>]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grouped = data.groupby('class')\n",
    "rowlength = grouped.ngroups/2                         # fix up if odd number of groups\n",
    "fig, axs = plt.subplots(figsize=(9,4), \n",
    "                        nrows=2, ncols=rowlength,     # fix as above\n",
    "                        gridspec_kw=dict(hspace=0.4)) # Much control of gridspec\n",
    "\n",
    "targets = zip(grouped.groups.keys(), axs.flatten())\n",
    "print targets\n",
    "grouped.get_group('18-24').hist(alpha=0.4)\n",
    "#for i, (key, ax) in enumerate(targets):\n",
    "#    ax.plot(grouped.get_group(key))\n",
    "#    ax.set_title('a=%s'%str(key))\n",
    "#ax.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = data.groupby('class')\n",
    "grouped.mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BAR PLOTS OF MEAN VALUE OF FEATURES FOR EACH CLASS ######\n",
    "\n",
    "grouped = data.groupby('class')\n",
    "plt.figure()\n",
    "grouped.mean().T.plot(kind='bar', figsize=(60,10))\n",
    "plt.savefig('test1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Distribution over a feature for each class #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "grouped = data.groupby('class')\n",
    "import numpy\n",
    "ncol = 4 # pick one dimension\n",
    "nrow = (len(feature_names)+ ncol-1) / ncol # make sure enough subplots\n",
    "#fig, ax = plt.subplots(nrows=nrow, ncols=ncol, figsize=(6,6)) # create the axes\n",
    "j = 0\n",
    "for key in list(data.columns.values):\n",
    "#    ix = numpy.unravel_index(j, ax.shape)\n",
    "#    print ix\n",
    "    print key\n",
    "    if key!='class':\n",
    "        j += 1\n",
    "        plt.figure(j, figsize=(10,10))\n",
    "        grouped[key].plot(kind='kde', alpha=0.8, legend=grouped.groups.keys(), title=key)\n",
    "    #g = grouped[key]\n",
    "    #print grouped[key].mean()\n",
    "    #if j==1:\n",
    "    #    tmp = g.mean()\n",
    "    #else:\n",
    "    #    print g.mean()\n",
    "    #    tmp.append(g.mean())\n",
    "    #print tmp\n",
    "        plt.show()\n",
    "    #if j==2:\n",
    "    #    break\n",
    "#tmp\n",
    "    #break\n",
    "    #ax[ix] = grouped[key].plot(kind='kde', alpha=0.4, legend=grouped.groups.keys())\n",
    "    #break\n",
    "#for key in grouped.keys:\n",
    "#    grouped[key].plot(kind='kde', alpha=0.4, legend=grouped.groups.keys())\n",
    "#for key in grouped.groups.keys():\n",
    "#    b = grouped.get_group(key)\n",
    "#    b.plot('kin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "ncol = 4 # pick one dimension\n",
    "nrow = (len(feature_names)+ ncol-1) / ncol # make sure enough subplots\n",
    "fig, ax = plt.subplots(nrows=nrow, ncols=ncol) # create the axes\n",
    "j = 0\n",
    "for i in feature_names: \n",
    "    ix = numpy.unravel_index(j, ax.shape)\n",
    "    #print ix\n",
    "    j += 1\n",
    "    ax[ix] = data.groupby('class').i.hist(alpha=0.4)   # go over a linear list of data # compute an appropriate index (1d or 2d)\n",
    "    #feat = feature_names[i]\n",
    "    #data.groupby('class').feat.hist(alpha=0.4, ax=ax[i])\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib outline\n",
    "plt.savefig('CameraEvolution.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = grid_search.best_estimator_.steps[1][1]\n",
    "#import pydot\n",
    "import pyparsing\n",
    "\n",
    "#reload(pydot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint, numpy\n",
    "from operator import itemgetter\n",
    "\n",
    "feat_importance = zip(list(numpy.array(feature_names)[numpy.nonzero(clf.feature_importances_)]), list(clf.feature_importances_[numpy.nonzero(clf.feature_importances_)]))\n",
    "feat_importance = sorted(feat_importance, key=itemgetter(1))[::-1]\n",
    "feat_importance\n",
    "#for i in zip(list(numpy.array(feature_names)[numpy.nonzero(clf.feature_importances_)]), list(clf.feature_importances_([numpy.nonzero(clf.feature_importances_)]))):\n",
    "#    i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f, feature_names=feature_names,\n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "#>>> import os\n",
    "#>>> os.unlink('iris.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.externals.six import StringIO\n",
    "from sklearn import tree\n",
    "import pydot\n",
    ">>> from IPython.display import Image  \n",
    ">>> dot_data = StringIO()  \n",
    ">>> tree.export_graphviz(clf,  out_file=dot_data,\n",
    "                         feature_names=feature_names,\n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    ">>> graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "#>>> Image(graph.create_png())   \n",
    ">>> graph.write_pdf(\"iris.pdf\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
