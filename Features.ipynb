{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset->Grouping User texts.\n",
      "\n",
      "Loaded 436 users...\n",
      "\n",
      "\n",
      "--------------- Thy time of Running ---------------\n",
      "Learning to judge age..\n",
      "Learning to judge gender..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.externals import joblib\n",
    "from tictacs import from_recipe\n",
    "from pan import ProfilingDataset\n",
    "import dill\n",
    "import cPickle as pickle\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "infolder = \"../DATA/pan16-author-profiling-training-dataset-2016-04-25/pan16-author-profiling-training-dataset-english-2016-02-29/\"\n",
    "outfolder = \"models/\"\n",
    "print('Loading dataset->Grouping User texts.\\n')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded {} users...\\n'.format(len(dataset.entries)))\n",
    "# get config\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "for task in tasks:\n",
    "    print('Learning to judge %s..' % task)\n",
    "    # load data\n",
    "    X, y = dataset.get_data(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277792\n"
     ]
    }
   ],
   "source": [
    "from pan import ProfilingDataset, createDocProfiles, create_target_prof_trainset\n",
    "from pan import preprocess\n",
    "\n",
    "task = 'age'\n",
    "docs = createDocProfiles(dataset)\n",
    "X, y = create_target_prof_trainset(docs, task)\n",
    "print len(X)\n",
    "#X = preprocess.preprocess(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "#reload(preprocess)\n",
    "#reload(features)\n",
    "from pan import features\n",
    "from pan import preprocess\n",
    "X, y = dataset.get_data('age')\n",
    "#X, y = dataset.get_data('gender')\n",
    "print len(X)\n",
    "#print X[0]\n",
    "#X = preprocess.preprocess(X)\n",
    "#print \"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\"\n",
    "#print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = X[11000]\n",
    "c = CharFeatureExtractor(True)\n",
    "tr = c.transform(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "import numpy\n",
    "from textblob.tokenizers import WordTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pan.misc import _twokenize\n",
    "\n",
    "class CharFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\" Model that extracts Char Based Features. Counter: 32 features \"\"\"\n",
    "\n",
    "    def __init__(self, norm, norm_axis=0):\n",
    "        self.letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
    "                  'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "                  'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        self.numbers = ['0', '1', '2', '3','4', '5', '6', '7', '8', '9']\n",
    "        self.special_c = ['!', '.', ':', '?', ';', ',', ')', '(', \n",
    "                          '-', '%', '$', '#', '@', '^', '&', '*', \n",
    "                          '=', '+', '/', '\"', \"'\", '<', '>', '|',\n",
    "                          '~', '`']\n",
    "        self.names = ['#C', '#a-z', '#upper', '#0-9', '#w', '#\\t'] + ['No_'+str(i) for i in self.special_c]\n",
    "        self.norm = norm\n",
    "        self.norm_axis = norm_axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\" transform data\n",
    "\n",
    "        :texts: The texts to count capital words in\n",
    "        :returns: array\n",
    "\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import normalize\n",
    "        \n",
    "        final_feat = []\n",
    "        for text in texts:\n",
    "            #text = text.encode('utf-8', 'ignore')\n",
    "            tmp = []\n",
    "            tmp.append(len(text))\n",
    "            if tmp[0] == 0:\n",
    "                tmp = [0 for i in self.names]\n",
    "            else:\n",
    "                tmp.append(sum([text.lower().count(l) for l in self.letters])/float(tmp[0]))\n",
    "                tmp.append(sum([l.isupper() for l in text])/float(tmp[0]))\n",
    "                tmp.append(sum([text.lower().count(l) for l in self.numbers])/float(tmp[0]))\n",
    "                tmp.append(text.count(' ')/float(tmp[0]))\n",
    "                tmp.append(text.count('\\t')/float(tmp[0]))\n",
    "                tmp.extend([text.lower().count(l)/float(tmp[0]) for l in self.special_c])\n",
    "            final_feat.append(tmp)\n",
    "        if self.norm:\n",
    "            return normalize(numpy.array(final_feat), norm='l1', axis=self.norm_axis)\n",
    "        else:\n",
    "            return numpy.array(final_feat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_p = './English_Function_Words_Set/'\n",
    "func_w = []\n",
    "for txt in sorted(os.listdir(folder_p)):\n",
    "    with open(folder_p+txt, 'r') as inp:\n",
    "        func_w.extend(inp.readlines())\n",
    "func_w = sorted(list(set([f.strip('\\n').strip('\\r') for f in func_w])))\n",
    "#with open('./English_Function_Words_Set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pan.features import tokenization, tokenization2\n",
    "from pan.misc import _twokenize\n",
    "import math\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "def get_yules(bow):\n",
    "    \"\"\" \n",
    "    Returns  Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        token_counter = collections.Counter(tok.upper() for tok in bow)\n",
    "        m1 = sum(token_counter.values())\n",
    "        m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "        i = (m1*m1) / (m2-m1)\n",
    "        k = 1/i * 10000\n",
    "    except ZeroDivisionError:\n",
    "        k = 0\n",
    "        i = 0\n",
    "    return [k, i]\n",
    "\n",
    "def get_simpsons(bow):\n",
    "    \"\"\" \n",
    "    Returns the value of Simpsons D and entropy, for this bow list.\n",
    "    \"\"\"\n",
    "    unique = list(set(bow))\n",
    "    unique_occ = [bow.count(u_w) for u_w in unique]\n",
    "    d = 0\n",
    "    e = 0\n",
    "    for ind, u_w in enumerate(unique):\n",
    "        try:\n",
    "            d += unique_occ.count(unique_occ[ind])*unique_occ[ind]*(unique_occ[ind]-1)/float(((len(bow))*(len(bow)-1)))\n",
    "        except ZeroDivisionError:\n",
    "            d += 0\n",
    "        try:\n",
    "            e += -unique_occ.count(unique_occ[ind])*unique_occ[ind]*math.log(unique_occ[ind]/float(len(bow)),10)/float(len(bow))\n",
    "        except ZeroDivisionError:\n",
    "            e += 0\n",
    "    return [d, e]\n",
    "\n",
    "def get_honores(bow):\n",
    "    \"\"\" \n",
    "    Returns the value of Sichels S, for this bow list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ret =float((100*math.log(len(bow), 10))) / (1 - (sum([1 for w in bow if bow.count(w)==2])/ len(set(bow))))\n",
    "    except ZeroDivisionError:\n",
    "        ret = 0\n",
    "    return ret\n",
    "\n",
    "        \n",
    "def get_wl_distr(bow, bins=20):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    tmp = [0 for wl in xrange(bins)]\n",
    "    for w in bow:\n",
    "        for wl in xrange(bins):\n",
    "            if len(w) == wl:\n",
    "                tmp[wl] += 1\n",
    "                break\n",
    "    return [i/float(len(bow)) for i in tmp]\n",
    "\n",
    "def get_function_words(folder_p):\n",
    "    func_w = []\n",
    "    for txt in sorted(os.listdir(folder_p)):\n",
    "        with open(folder_p+txt, 'r') as inp:\n",
    "            func_w.extend(inp.readlines())\n",
    "    func_w = sorted(list(set([f.strip('\\n').strip('\\r') for f in func_w])))\n",
    "    return func_w\n",
    "\n",
    "class WordFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\" Model that extracts Word Based Features. Counter: 32 features \"\"\"\n",
    "\n",
    "    def __init__(self, function_words_path='./English_Function_Words_Set/', norm=True, norm_axis=0, tokenizer_var='twitter', bins=20):\n",
    "        if not(function_words_path):\n",
    "            function_words_path = './English_Function_Words_Set/'\n",
    "        self.func_w = get_function_words(function_words_path)\n",
    "        self.tokenizer_var = tokenizer_var\n",
    "        if self.tokenizer_var == 'token1':\n",
    "            self.tokenization = tokenization\n",
    "        elif self.tokenizer_var == 'token2':\n",
    "            self.tokenization = tokenization2\n",
    "        elif self.tokenizer_var == 'twitter':\n",
    "            self.tokenization = _twokenize.tokenizeRawTweetText\n",
    "        else:\n",
    "            self.tokenization = _twokenize.tokenizeRawTweetText\n",
    "        self.norm = norm\n",
    "        self.norm_axis = norm_axis\n",
    "        self.bins = 20\n",
    "        self.names = ['#N', 'avg_wl', 'voc_rich', '#wl>6', '#wl<3', '#hapax',\n",
    "                       '#hapax2', 'yule_k', 'yule_i', 'simp_d', 'entropy',\n",
    "                       'sich_s', 'honore'] + ['fr_wl_'+ str(i+1) for i in xrange(bins)] + self.func_w\n",
    "                    \n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\" transform data\n",
    "\n",
    "        :texts: The texts to count capital words in\n",
    "        :returns: array\n",
    "\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import normalize\n",
    "        \n",
    "        final_feat = []\n",
    "        for text in texts:\n",
    "            bow = self.tokenization(text)\n",
    "            #text = text.encode('utf-8', 'ignore')\n",
    "            tmp = []\n",
    "            tmp.append(len(bow))\n",
    "            #print bow\n",
    "            #print len(set(bow))\n",
    "            if tmp[0] == 0:\n",
    "                tmp = [0 for i in self.names]\n",
    "            else:\n",
    "                tmp.append(sum([len(w) for w in bow])/float(tmp[0]))\n",
    "                tmp.append(len(set(bow))/float(tmp[0]))\n",
    "                tmp.append(sum([1 for w in bow if len(w)> 6])/float(tmp[0]))\n",
    "                tmp.append(sum([1 for w in bow if len(w)<=3])/float(tmp[0]))\n",
    "                tmp.append(sum([1 for w in bow if bow.count(w)==1])/float(tmp[0]))\n",
    "                tmp.append(sum([1 for w in bow if bow.count(w)==2])/float(tmp[0]))\n",
    "                tmp.extend(get_yules(bow))\n",
    "                tmp.extend(get_simpsons(bow))\n",
    "                tmp.append(sum([1 for w in bow if bow.count(w)==2])/float(len(set(bow))))\n",
    "                tmp.append(get_honores(bow))\n",
    "                tmp.extend(get_wl_distr(bow, self.bins))\n",
    "                tmp.extend([bow.count(w)/float(len(bow)) for w in self.func_w])\n",
    "            final_feat.append(tmp)\n",
    "            #print len(tmp)\n",
    "            #print len(self.names)\n",
    "            #print self.names\n",
    "            #print tmp\n",
    "        #print len(final_feat)\n",
    "        if self.norm:\n",
    "            return normalize(numpy.array(final_feat), norm='l1', axis=self.norm_axis)\n",
    "        else:\n",
    "            return numpy.array(final_feat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Come to &quot;Algo M\\xe1s Inesperado Que La Muerte&quot; Wednesday, March 24 at 8:00 pm until &lt;br /&gt;Saturday, April 24 at 11:00... http://fb.me/7kLR3SI'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = _twokenize.tokenizeRawTweetText('Lala this is funny! Lala! :D. :(')\n",
    "aa = WordFeatureExtractor('./English_Function_Words_Set/')\n",
    "tr = aa.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  #N         avg_wl       voc_rich          #wl>6  \\\n",
      "count  222233.000000  222233.000000  222233.000000  222233.000000   \n",
      "mean        0.000004       0.000004       0.000004       0.000004   \n",
      "std         0.000003       0.000002       0.000002       0.000002   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000002       0.000004       0.000003       0.000004   \n",
      "50%         0.000005       0.000004       0.000003       0.000004   \n",
      "75%         0.000006       0.000005       0.000006       0.000005   \n",
      "max         0.000055       0.000147       0.000009       0.000029   \n",
      "\n",
      "              #wl<3         #hapax        #hapax2         yule_k  \\\n",
      "count  2.222330e+05  222233.000000  222233.000000  222233.000000   \n",
      "mean   4.499782e-06       0.000004       0.000004       0.000004   \n",
      "std    8.749920e-07       0.000003       0.000004       0.000612   \n",
      "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
      "25%    4.450341e-06       0.000002       0.000002       0.000000   \n",
      "50%    4.688602e-06       0.000003       0.000004       0.000000   \n",
      "75%    4.956973e-06       0.000006       0.000006       0.000000   \n",
      "max    7.216770e-06       0.000012       0.000044       0.083333   \n",
      "\n",
      "              yule_i         simp_d      ...             fr_wl_11  \\\n",
      "count  222233.000000  222233.000000      ...        222233.000000   \n",
      "mean        0.000004       0.000004      ...             0.000004   \n",
      "std         0.000007       0.000002      ...             0.000012   \n",
      "min         0.000000       0.000000      ...             0.000000   \n",
      "25%         0.000002       0.000003      ...             0.000000   \n",
      "50%         0.000002       0.000005      ...             0.000000   \n",
      "75%         0.000004       0.000006      ...             0.000005   \n",
      "max         0.000100       0.000058      ...             0.000603   \n",
      "\n",
      "            fr_wl_12       fr_wl_13       fr_wl_14       fr_wl_15  \\\n",
      "count  222233.000000  222233.000000  222233.000000  222233.000000   \n",
      "mean        0.000004       0.000004       0.000004       0.000004   \n",
      "std         0.000006       0.000006       0.000011       0.000008   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000003       0.000005       0.000000       0.000004   \n",
      "75%         0.000007       0.000008       0.000007       0.000007   \n",
      "max         0.000335       0.000446       0.000755       0.000781   \n",
      "\n",
      "            fr_wl_16       fr_wl_17       fr_wl_18       fr_wl_19  \\\n",
      "count  222233.000000  222233.000000  222233.000000  222233.000000   \n",
      "mean        0.000004       0.000004       0.000004       0.000004   \n",
      "std         0.000005       0.000045       0.000005       0.000036   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000003       0.000000       0.000005       0.000000   \n",
      "75%         0.000008       0.000000       0.000006       0.000000   \n",
      "max         0.000411       0.006869       0.000312       0.005157   \n",
      "\n",
      "            fr_wl_20  \n",
      "count  222233.000000  \n",
      "mean        0.000004  \n",
      "std         0.000031  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         0.004315  \n",
      "\n",
      "[8 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(tr, columns=aa.names)\n",
    "data[\"class\"] = y_train\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = data.groupby('class')\n",
    "means = grouped.mean().T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### BAR PLOTS OF MEAN VALUE OF FEATURES FOR EACH CLASS ######\n",
    "\n",
    "grouped = data.groupby('class')\n",
    "#plt.figure()\n",
    "grouped.mean().T.plot(kind='bar', figsize=(60,10))\n",
    "plt.savefig('test1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "c = CharFeatureExtractor(True)\n",
    "w = WordFeatureExtractor(function_words_path='./English_Function_Words_Set/', norm=True, norm_axis=0)\n",
    "svm = LinearSVC(C=0.001, dual=False, class_weight='balanced')\n",
    "svm2 = SVC(kernel='rbf', C=1, gamma=1, class_weight='balanced', probability=False)\n",
    "pipe1 = Pipeline([('CharCount', c), ('svm', svm)])\n",
    "pipe2 = Pipeline([('WordCount', w), ('svm', svm2)])\n",
    "\n",
    "num_folds = 3\n",
    "split = 0.2\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, stratify=y, random_state=100)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "#eclf = VotingClassifier(estimators=[(\"0\", pipe1), ('1', pipe2)], voting='soft')\n",
    "pipe2.fit(X_train)\n",
    "# trained_models = []\n",
    "# params = {}\n",
    "# params = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "# for model in [pipe2]:\n",
    "#     grid_search = GridSearchCV(estimator=pipe2, param_grid=params, verbose=1, n_jobs=2, cv=num_folds, refit=True)\n",
    "#     grid_search.fit(X_train,y_train)\n",
    "#     print(grid_search.best_score_)\n",
    "#     print(grid_search.best_estimator_) \n",
    "# pipe2 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.411904461923\n",
      "Confusion matrix :\n",
      " [[    0     0  3842     0     0]\n",
      " [    0     0 19244     0     0]\n",
      " [    0     0 22885     0     0]\n",
      " [    0     0  9085     0     0]\n",
      " [    0     0   503     0     0]]\n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      18-24       0.00      0.00      0.00      3842\n",
      "      25-34       0.00      0.00      0.00     19244\n",
      "      35-49       0.41      1.00      0.58     22885\n",
      "      50-64       0.00      0.00      0.00      9085\n",
      "      65-xx       0.00      0.00      0.00       503\n",
      "\n",
      "avg / total       0.17      0.41      0.24     55559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = pipe1.predict(X_cv)\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=sorted(list(set(y_cv))))\n",
    "rep = classification_report(y_cv, predict, target_names=sorted(list(set(y_cv))))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))\n",
    "print('Classification report :\\n {}'.format(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
