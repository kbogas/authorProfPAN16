{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset->Grouping User texts.\n",
      "\n",
      "Loaded 436 users...\n",
      "\n",
      "\n",
      "--------------- Thy time of Running ---------------\n",
      "Learning to judge age..\n",
      "Learning to judge gender..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.externals import joblib\n",
    "from tictacs import from_recipe\n",
    "from pan import ProfilingDataset\n",
    "import dill\n",
    "import cPickle as pickle\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "infolder = \"../DATA/pan16-author-profiling-training-dataset-2016-04-25/pan16-author-profiling-training-dataset-english-2016-02-29/\"\n",
    "outfolder = \"models/\"\n",
    "print('Loading dataset->Grouping User texts.\\n')\n",
    "dataset = ProfilingDataset(infolder)\n",
    "print('Loaded {} users...\\n'.format(len(dataset.entries)))\n",
    "# get config\n",
    "config = dataset.config\n",
    "tasks = config.tasks\n",
    "print('\\n--------------- Thy time of Running ---------------')\n",
    "for task in tasks:\n",
    "    print('Learning to judge %s..' % task)\n",
    "    # load data\n",
    "    X, y = dataset.get_data(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277792\n"
     ]
    }
   ],
   "source": [
    "from pan import ProfilingDataset, createDocProfiles, create_target_prof_trainset\n",
    "from pan import preprocess\n",
    "\n",
    "task = 'gender'\n",
    "docs = createDocProfiles(dataset)\n",
    "X, y = create_target_prof_trainset(docs, task)\n",
    "print len(X)\n",
    "#X = preprocess.preprocess(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "#reload(preprocess)\n",
    "#reload(features)\n",
    "from pan import features\n",
    "from pan import preprocess\n",
    "#X, y = dataset.get_data('age')\n",
    "X, y = dataset.get_data('gender')\n",
    "print len(X)\n",
    "#print X[0]\n",
    "#X = preprocess.preprocess(X)\n",
    "#print \"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\"\n",
    "#print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../DATA/graph-twitter/graph_gender_age/X.txt\", 'r') as inp:\n",
    "    X = pickle.load(inp)\n",
    "with open(\"../DATA/graph-twitter/graph_gender_age/y.txt\", 'r') as yinp:\n",
    "    y = pickle.load(yinp)\n",
    "X = preprocess.preprocess(X)\n",
    "y = [s.lower() for s in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 303\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../DATA/graph-twitter/graph_gender_age/labeled.json\", 'r') as inp:\n",
    "    labeled = json.load(inp)\n",
    "X = []\n",
    "y = []\n",
    "for user in labeled:\n",
    "    y.append(user['gender'].lower())\n",
    "    X.append(' '.join(user['texts']))\n",
    "# for user in labeled:\n",
    "#     y.extend([user['gender'].lower() for t in user['texts']])\n",
    "#     X.extend(user['texts'])\n",
    "print len(X), len(y)\n",
    "#X = preprocess.preprocess(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SOACWrapper(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    import time\n",
    "    import numpy\n",
    "    from sklearn.preprocessing import normalize\n",
    "    from collections import Counter\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.labels = []\n",
    "        self.prior_row = None\n",
    "        self.term_table = None\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        import time\n",
    "        import numpy\n",
    "        from sklearn.preprocessing import normalize\n",
    "        from collections import Counter\n",
    "        \n",
    "        if y is None:\n",
    "            raise ValueError('we need y labels to supervise-fit!')\n",
    "        else:\n",
    "            time_start = time.time()\n",
    "            target_profiles = sorted(list(set(y)))\n",
    "            self.labels = target_profiles\n",
    "            ##SOA\n",
    "            doc_prof = numpy.zeros([X.shape[0], len(target_profiles)])\n",
    "            for i in range(0, X.shape[0]):\n",
    "                tmp = numpy.zeros([1, len(target_profiles)])\n",
    "                tmp[0, target_profiles.index(y[i])] = 1\n",
    "                doc_prof[i, :] = tmp\n",
    "            \n",
    "            ## SOAC\n",
    "#            dd = Counter(y)\n",
    "#             self.prior_row = numpy.zeros([1, len(target_profiles)])\n",
    "#             for i, key in enumerate(sorted(dd.keys())):\n",
    "#                 dd[key] = dd[key]/float(len(y))\n",
    "#                 self.prior_row[0, i] = 1/dd[key]\n",
    "#             doc_prof = numpy.tile(self.prior_row, (X.shape[0], 1))\n",
    "#             for i in range(0, X.shape[0]):\n",
    "#                 doc_prof[i, target_profiles.index(y[i])] = 0\n",
    "#\n",
    "            #print self.prior_row\n",
    "            # Gia to palio. Na vgalw kai doc_prof apo to transform\n",
    "            #self.prior_row = numpy.ones([1, len(target_profiles)])\n",
    "            \n",
    "            #try:\n",
    "                #X = numpy.log2(X + 1)\n",
    "            #except Exception, e:\n",
    "            #    print \"Error in log2\"\n",
    "            #    print e\n",
    "            print 'X'\n",
    "            print X.shape\n",
    "            print X\n",
    "            print 'Doc_prof'\n",
    "            print doc_prof.shape\n",
    "            print doc_prof\n",
    "            try:\n",
    "                term_prof = 100*X.transpose().dot(doc_prof)\n",
    "            except Exception, e:\n",
    "                print \"Error in product\"\n",
    "                print e\n",
    "            print 'Term_prof'\n",
    "            print term_prof.shape\n",
    "            print term_prof\n",
    "#             print 'Dot Product'\n",
    "#             print term_prof\n",
    "            term_prof = term_prof / term_prof.sum(axis=0)\n",
    "            print 'First'\n",
    "            print term_prof\n",
    "            #print \"Normalization per collumn product in %0.2f sec\" % (time.time()- time_start)\n",
    "            #normalize(term_prof, norm='l1', axis=0, copy=False)\n",
    "            # normalize across profiles\n",
    "            print 'Second'\n",
    "            term_prof = normalize(term_prof, norm='l1', axis=1, copy=False)\n",
    "            #print term_prof\n",
    "#             term_prof = term_prof / \\\n",
    "#                numpy.reshape(\n",
    "#                   term_prof.sum(axis=1), (term_prof.sum(axis=1).shape[0], 1))\n",
    "            print term_prof\n",
    "            #print \"Normalization per collumn product in %0.2f sec\" % (time.time()- time_start)\n",
    "#             print 'First'\n",
    "#             term_prof = normalize(term_prof, norm='l1', axis=0, copy=False)\n",
    "#             print term_prof\n",
    "#             print 'Second'\n",
    "#             term_prof = normalize(term_prof, norm='l1', axis=1, copy=False)\n",
    "#             print term_prof\n",
    "            # clean term_prof\n",
    "            # term_prof = cleaned(term_prof, self.thres)\n",
    "            self.term_table = term_prof\n",
    "            return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        import numpy\n",
    "        from sklearn.preprocessing import normalize\n",
    "        \n",
    "        if self.labels is None:\n",
    "            raise AttributeError('term_table was no found! \\\n",
    "                     Probably model was not fitted first. Run model.fit(X,y)!')\n",
    "            exit(1)\n",
    "        else:\n",
    "            doc_prof = X.dot(self.term_table)\n",
    "            for i in range(0, doc_prof.shape[0]):\n",
    "                doc_prof[i, :] = doc_prof[i, :] #- doc_prof[i, :].min()\n",
    "            # print doc_prof\n",
    "            #normalize(doc_prof, norm='l1', axis=1, copy=False)\n",
    "            return doc_prof\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        import numpy\n",
    "        num2label = dict((i, label) for i, label in enumerate(self.labels))\n",
    "        Y = self.transform(X)\n",
    "        y = numpy.argmax(Y, axis=1)\n",
    "        final = numpy.array([num2label[i] for i in y])\n",
    "#         for i,_ in enumerate(final):\n",
    "#             print 'X'\n",
    "#             print X[i,:]\n",
    "#             print 'TR'\n",
    "#             print Y[i,:]\n",
    "#             print 'Max || Predict || Real'\n",
    "#             print y[i], final[i], y_true[i]\n",
    "        return final\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.preprocessing import normalize\n",
    "a = numpy.random.rand(2,2)\n",
    "print a\n",
    "normalize(a, norm='l1', axis=1, copy=False)`\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.max(tr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = X[11000]\n",
    "c = CharFeatureExtractor(True)\n",
    "#tr = c.transform(X_train)\n",
    "soacw = SOACWrapper()\n",
    "pipe3 = Pipeline([('CharCount', c), ('soac', soacw)])\n",
    "#soacw.fit(tr, y_train)\n",
    "gg = pipe3.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe3.steps[1][1].term_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = CharFeatureExtractor(True)\n",
    "#tr = c.transform(X_train)\n",
    "tr[4,:].dot(pipe3.steps[1][1].term_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = soacw.predict(c.transform(X_cv), y_cv)\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=sorted(list(set(y_cv))))\n",
    "rep = classification_report(y_cv, predict, target_names=sorted(list(set(y_cv))))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))\n",
    "print('Classification report :\\n {}'.format(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in index:\n",
    "    print predict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in xrange(len(y_train)):\n",
    "    if y_train[i] == '65-xx':\n",
    "        print i\n",
    "        index.append(i)\n",
    "index.extend([9, 15, 28,40])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c2 = CharFeatureExtractor(True)\n",
    "tr3 =c2.transform([X_train[i] for i in index])\n",
    "for i in tr3:\n",
    "    print tr3.argmax(), tr3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "import numpy\n",
    "from textblob.tokenizers import WordTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pan.misc import _twokenize\n",
    "\n",
    "class CharFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\" Model that extracts Char Based Features. Counter: 32 features \"\"\"\n",
    "\n",
    "    def __init__(self, norm, norm_axis=0):\n",
    "        self.letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
    "                  'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "                  'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        self.numbers = ['0', '1', '2', '3','4', '5', '6', '7', '8', '9']\n",
    "        self.special_c = ['!', '.', ':', '?', ';', ',', ')', '(', \n",
    "                          '-', '%', '$', '#', '@', '^', '&', '*', \n",
    "                          '=', '+', '/', '\"', \"'\", '<', '>', '|',\n",
    "                          '~', '`']\n",
    "        self.names = ['#C', '#a-z', '#upper', '#0-9', '#w', '#\\t'] + ['No_'+str(i) for i in self.special_c]\n",
    "        self.norm = norm\n",
    "        self.norm_axis = norm_axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\" transform data\n",
    "\n",
    "        :texts: The texts to count capital words in\n",
    "        :returns: array\n",
    "\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import normalize\n",
    "        \n",
    "        final_feat = []\n",
    "        for text in texts:\n",
    "            #text = text.encode('utf-8', 'ignore')\n",
    "            tmp = []\n",
    "            tmp.append(len(text))\n",
    "            if tmp[0] == 0:\n",
    "                tmp = [0 for i in self.names]\n",
    "            else:\n",
    "                tmp.append(sum([text.lower().count(l) for l in self.letters])/float(tmp[0]))\n",
    "                tmp.append(sum([l.isupper() for l in text])/float(tmp[0]))\n",
    "                tmp.append(sum([text.lower().count(l) for l in self.numbers])/float(tmp[0]))\n",
    "                tmp.append(text.count(' ')/float(tmp[0]))\n",
    "                tmp.append(text.count('\\t')/float(tmp[0]))\n",
    "                tmp.extend([text.lower().count(l)/float(tmp[0]) for l in self.special_c])\n",
    "            final_feat.append(tmp)\n",
    "        if self.norm:\n",
    "            final_feat2 = normalize(numpy.array(final_feat), norm='l1', axis=0)\n",
    "            #final_feat2 = normalize(numpy.array(final_feat), norm='l1', axis=1)\n",
    "            return final_feat2\n",
    "            #return normalize(numpy.array(final_feat), norm='l1', axis=self.norm_axis)\n",
    "        else:\n",
    "            return numpy.array(final_feat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "import numpy\n",
    "from textblob.tokenizers import WordTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pan.misc import _twokenize\n",
    "\n",
    "eyes = [':', ';']\n",
    "noses = ['', '-', '=', 'o', 'O', '0', '>']\n",
    "mouths = [')', 'D', ']', '>',  '(',  '[', '<', 'p', 'P', 'd', \"|\", '/', '*', '@', '!', '$', '3', 'X']\n",
    "emoticons = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\", \"=}\",\"(:;)\", \"{;:]\", \"^_^\", \"^-^\"]\n",
    "for eye in eyes:\n",
    "    for nose in noses:\n",
    "        for mouth in mouths:\n",
    "            emoticons.append(eye+nose+mouth)\n",
    "\n",
    "\n",
    "eyes = [':', ';']\n",
    "noses = ['', '-']\n",
    "mouths = [')', 'D', ']', '>',  '(',  '[', '<', 'p', 'P', 'd']            \n",
    "tongue_no_emo = []\n",
    "for nose in noses:\n",
    "    for eye in eyes:\n",
    "        for mouth in mouths:\n",
    "            tongue_no_emo.append(eye+nose+mouth)\n",
    "    \n",
    "class EmoticonExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\" Model that extracts Char Based Features. Counter: 32 features \"\"\"\n",
    "\n",
    "    def __init__(self, emoticons, norm=True, norm_axis=0):\n",
    "        self.emoticons = emoticons\n",
    "        self.names = emoticons\n",
    "        self.norm = norm\n",
    "        self.norm_axis = norm_axis\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\" transform data\n",
    "\n",
    "        :texts: The texts to count capital words in\n",
    "        :returns: array\n",
    "\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import normalize\n",
    "        \n",
    "        final_feat = []\n",
    "        for text in texts:\n",
    "            #text = text.encode('utf-8', 'ignore')\n",
    "            tmp = [text.count(emo) for emo in self.emoticons]\n",
    "            #if ';od' in text:\n",
    "            #    print 'Kai omws'\n",
    "            final_feat.append(tmp)\n",
    "        final_feat2 = numpy.array(final_feat)\n",
    "        final_feat3 = final_feat2[:, numpy.nonzero(final_feat2.sum(axis=0))[0]]\n",
    "        print 'Indexes'\n",
    "        print numpy.nonzero(final_feat2.sum(axis=0))[0]\n",
    "        print type(numpy.nonzero(final_feat2.sum(axis=0))[0])\n",
    "        print 'Names'\n",
    "        print numpy.array(self.names).shape\n",
    "        self.names = numpy.array(self.names)[numpy.nonzero(final_feat2.sum(axis=0))[0]]\n",
    "        print final_feat3.shape\n",
    "        if self.norm:\n",
    "            final_feat3 = normalize(numpy.array(final_feat3), norm='l1', axis=0)\n",
    "            return final_feat3\n",
    "            #return normalize(numpy.array(final_feat), norm='l1', axis=self.norm_axis)\n",
    "        else:\n",
    "            return final_feat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 88 436 436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "split = 0.2\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, stratify=y, random_state=100)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "aa = EmoticonExtractor(emoticons)\n",
    "tr = aa.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = _twokenize.tokenizeRawTweetText('Lala this is funny! Lala! :D. :(')\n",
    "aa = WordFeatureExtractor('./English_Function_Words_Set/')\n",
    "tr = aa.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = CharFeatureExtractor(True)\n",
    "tr = c.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = pipe3.steps[1][1].transform(pipe3.steps[0][1].transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(pipe3.steps[1][1].term_table, columns=['soa_%d'% i for i in xrange(len(set(y)))])\n",
    "#data[\"class\"] = y_train\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               *O         * *         ^_^          :)          :D          :]  \\\n",
      "count  348.000000  348.000000  348.000000  348.000000  348.000000  348.000000   \n",
      "mean     0.002874    0.002874    0.002874    0.002874    0.002874    0.002874   \n",
      "std      0.028260    0.034671    0.031952    0.006732    0.014778    0.032748   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    0.000000    0.000319    0.000000    0.000000   \n",
      "75%      0.000000    0.000000    0.000000    0.002435    0.000000    0.000000   \n",
      "max      0.400000    0.600000    0.500000    0.056691    0.175610    0.500000   \n",
      "\n",
      "               :(          :[          :<          :p     ...             ;-3  \\\n",
      "count  348.000000  348.000000  348.000000  348.000000     ...      348.000000   \n",
      "mean     0.002874    0.002874    0.002874    0.002874     ...        0.002874   \n",
      "std      0.007759    0.039909    0.008030    0.015635     ...        0.053606   \n",
      "min      0.000000    0.000000    0.000000    0.000000     ...        0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000     ...        0.000000   \n",
      "50%      0.000000    0.000000    0.000930    0.000000     ...        0.000000   \n",
      "75%      0.001727    0.000000    0.002789    0.000000     ...        0.000000   \n",
      "max      0.070812    0.666667    0.084612    0.121212     ...        1.000000   \n",
      "\n",
      "              ;o)         ;op         ;od         ;OD         ;Op         ;OP  \\\n",
      "count  348.000000  348.000000  348.000000  348.000000  348.000000  348.000000   \n",
      "mean     0.002874    0.002874    0.002874    0.002874    0.002874    0.002874   \n",
      "std      0.037850    0.020558    0.037850    0.053606    0.014169    0.030860   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "max      0.500000    0.333333    0.500000    1.000000    0.137931    0.333333   \n",
      "\n",
      "              ;Od         ;0)         ;03  \n",
      "count  348.000000  348.000000  348.000000  \n",
      "mean     0.002874    0.002874    0.002874  \n",
      "std      0.053606    0.037850    0.053606  \n",
      "min      0.000000    0.000000    0.000000  \n",
      "25%      0.000000    0.000000    0.000000  \n",
      "50%      0.000000    0.000000    0.000000  \n",
      "75%      0.000000    0.000000    0.000000  \n",
      "max      1.000000    0.500000    1.000000  \n",
      "\n",
      "[8 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(tr, columns=aa.names)\n",
    "data[\"class\"] = y_train\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-73cf0692a3c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m### BAR PLOTS OF MEAN VALUE OF FEATURES FOR EACH CLASS ######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "grouped = data.groupby('class')\n",
    "means = grouped.mean().T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### BAR PLOTS OF MEAN VALUE OF FEATURES FOR EACH CLASS ######\n",
    "\n",
    "grouped = data.groupby('class')\n",
    "#plt.figure()\n",
    "grouped.sum().T.plot(kind='bar', figsize=(60,10), grid=True)\n",
    "plt.savefig('test1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.groupby.DataFrameGroupBy'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "';P'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(s):\n",
    "    return s/float(sum(s))\n",
    "\n",
    "print type(grouped)\n",
    "\n",
    "aaa = grouped.sum()\n",
    "aaa.loc['65-xx'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from pan.features import SOA_Model2, SOAC_Model2\n",
    "from pan import features\n",
    "\n",
    "grams3 = TfidfVectorizer(analyzer='word', ngram_range=[1,1], max_features=3000)\n",
    "soac = SOAC_Model2(max_df=0.9, min_df=5, tokenizer_var='sklearn', max_features=None)\n",
    "combined = FeatureUnion([('3grams', grams3), ('soac', soac)])\n",
    "svm = LinearSVC(C=100, class_weight='balanced')\n",
    "#svm = SVC(kernel='rbf', C=100, gamma=1, class_weight='balanced', probability=True)\n",
    "pipe = Pipeline([('combined',combined), ('svm', svm)])\n",
    "pipe.steps\n",
    "\n",
    "\n",
    "\n",
    "num_folds = 4\n",
    "split = 0.2\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, stratify=y, random_state=100)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "#eclf = VotingClassifier(estimators=[(\"0\", pipe1), ('1', pipe2)], voting='soft')\n",
    "#pipe2.fit(X_train, y_train)\n",
    "# trained_models = []\n",
    "# params = {}\n",
    "params = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "          'combined__soac__max_df': [1.0, 0.9, 0.8],\n",
    "          'combined__soac__max_features': [None, 5000, 10000],\n",
    "          'combined__soac__min_df': [1, 3, 5]}\n",
    "for model in [pipe]:\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, verbose=1, n_jobs=3, cv=num_folds, \n",
    "                                   refit=True, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_score_)\n",
    "    #print(grid_search.best_estimator_) \n",
    "    print('Best_params')\n",
    "    print(grid_search.grid_scores_)\n",
    "best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_params\n",
      "{'combined__soac__min_df': 3, 'svm__C': 1, 'combined__soac__max_df': 1.0, 'combined__soac__max_features': 5000}\n"
     ]
    }
   ],
   "source": [
    "print('Best_params')\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(242, 0)) while a minimum of 1 is required by the normalize function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-1e9a4b381777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-fe8e33390461>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_feat2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mfinal_feat3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_feat3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_feat3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m#return normalize(numpy.array(final_feat), norm='l1', axis=self.norm_axis)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     X = check_array(X, sparse_format, copy=copy, warn_on_dtype=True,\n\u001b[1;32m-> 1280\u001b[1;33m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m   1281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    413\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 415\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(242, 0)) while a minimum of 1 is required by the normalize function."
     ]
    }
   ],
   "source": [
    "tr = aa.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmoticonExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac99a7fcc590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0maa\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mEmoticonExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memoticons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCharFeatureExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msvm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EmoticonExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "aa =EmoticonExtractor(emoticons)\n",
    "c = CharFeatureExtractor(norm_axis=0, norm=True)\n",
    "svm2 = SVC(kernel='rbf', C=1, gamma=1, class_weight='balanced', probability=False)\n",
    "#w = WordFeatureExtractor(function_words_path='./English_Function_Words_Set/', norm=True, norm_axis=0)\n",
    "soacw = SOACWrapper()\n",
    "svm = LinearSVC(C=0.001,  class_weight='balanced')\n",
    "svm2 = SVC(kernel='linear', C=1, gamma=1, class_weight='balanced', probability=False)\n",
    "pipe1 = Pipeline([('CharCount', aa), ('svm', svm2)])\n",
    "#pipe2 = Pipeline([('WordCount', w), ('svm', svm2)])\n",
    "pipe2 = Pipeline([('CharCount', c), ('soac', soacw),('svm', svm2)])\n",
    "\n",
    "num_folds = 4\n",
    "split = 0.2\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=split, stratify=y, random_state=100)\n",
    "print len(X_train), len(X_cv), len(X_cv) + len(X_train), len(X)\n",
    "#eclf = VotingClassifier(estimators=[(\"0\", pipe1), ('1', pipe2)], voting='soft')\n",
    "#pipe2.fit(X_train, y_train)\n",
    "# trained_models = []\n",
    "# params = {}\n",
    "params = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "for model in [pipe2]:\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, verbose=1, n_jobs=2, cv=num_folds, \n",
    "                                   refit=True, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_estimator_) \n",
    "best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.475409836066\n",
      "Confusion matrix :\n",
      " [[29  0]\n",
      " [32  0]]\n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     female       0.48      1.00      0.64        29\n",
      "       male       0.00      0.00      0.00        32\n",
      "\n",
      "avg / total       0.23      0.48      0.31        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "predict = best.predict(X_cv)\n",
    "acc = accuracy_score(y_cv, predict)\n",
    "conf = confusion_matrix(y_cv, predict, labels=sorted(list(set(y_cv))))\n",
    "rep = classification_report(y_cv, predict, target_names=sorted(list(set(y_cv))))\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Confusion matrix :\\n {}'.format(conf))\n",
    "print('Classification report :\\n {}'.format(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "all_models = joblib.load(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
